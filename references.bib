@misc{fairness-survey,
      title={A Survey on Bias and Fairness in Natural Language Processing},
      author={Rajas Bansal},
      year={2022},
      eprint={2204.09591},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.09591}, 
}

@inproceedings{predictive-bias-NLP,
   title={Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview},
   url={http://dx.doi.org/10.18653/v1/2020.acl-main.468},
   DOI={10.18653/v1/2020.acl-main.468},
   booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
   publisher={Association for Computational Linguistics},
   author={Shah, Deven Santosh and Schwartz, H. Andrew and Hovy, Dirk},
   year={2020} }

@incollection{mill2012system,
  title={A system of logic},
  author={Mill, John Stuart},
  booktitle={Arguing about science},
  pages={243--267},
  year={2012},
  publisher={Routledge}
}

@article{WEAT,
    author = {Aylin Caliskan  and Joanna J. Bryson  and Arvind Narayanan },
    title = {Semantics derived automatically from language corpora contain human-like biases},
    journal = {Science},
    volume = {356},
    number = {6334},
    pages = {183-186},
    year = {2017},
    doi = {10.1126/science.aal4230},
    URL = {https://www.science.org/doi/abs/10.1126/science.aal4230},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.aal4230},
    abstract = {AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.}
}

@inproceedings{mitigating-bias-w-adversarial,
author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
title = {Mitigating Unwanted Biases with Adversarial Learning},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278779},
doi = {10.1145/3278721.3278779},
abstract = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {335–340},
numpages = {6},
keywords = {adversarial learning, debiasing, multi-task learning, unbiasing},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@article{IAT,
author = {Greenwald, A. G. and McGhee, D. E. and Schwartz, J. L. K.},
title = {Measuring individual differences in implicit cognition: The implicit association test},
year = {1998},
issue_date = {1998},
publisher = {Journal of Personality and Social Psychology},
address = {New York, NY, USA},
volume = {74},
number = {6},
url = {https://doi.org/10.1037/0022-3514.74.6.1464},
doi = {10.1037/0022-3514.74.6.1464},
abstract = {},
journal = {Journal of Personality and Social Psychology},
pages = {1-67}
}

@article{BERT,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{BIAS-BERT,
      title={Measuring Bias in Contextualized Word Representations}, 
      author={Keita Kurita and Nidhi Vyas and Ayush Pareek and Alan W Black and Yulia Tsvetkov},
      year={2019},
      eprint={1906.07337},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1906.07337}, 
}

@misc{SEAT,
      title={On Measuring Social Biases in Sentence Encoders}, 
      author={Chandler May and Alex Wang and Shikha Bordia and Samuel R. Bowman and Rachel Rudinger},
      year={2019},
      eprint={1903.10561},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1903.10561}, 
}

@misc{WEFE,
    title={Wefe: The word embeddings fairness evaluation framework},
    author={Badilla, Pablo and Bravo-Marquez, Felipe and P{\'e}rez, Jorge},
    year={2020},
    organization={International Joint Conferences on Artificial Intelligence}
}

@article{promt-learning-survey,
    author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
    title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
    year = {2023},
    issue_date = {September 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {55},
    number = {9},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3560815},
    doi = {10.1145/3560815},
    abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website  including constantly updated survey and paperlist.},
    journal = {ACM Comput. Surv.},
    month = {jan},
    articleno = {195},
    numpages = {35},
    keywords = {Pre-trained language models, prompting}
}

@inproceedings{PEZ,
 author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {51008--51025},
 publisher = {Curran Associates, Inc.},
 title = {Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a00548031e4647b13042c97c922fadf1-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{T-PGD,
      title={Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework}, 
      author={Lifan Yuan and Yichi Zhang and Yangyi Chen and Wei Wei},
      year={2023},
      eprint={2110.15317},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{LLM-explanation-survey,
author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
title = {Explainability for Large Language Models: A Survey},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3639372},
doi = {10.1145/3639372},
abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {feb},
articleno = {20},
numpages = {38},
keywords = {Explainability, interpretability, large language models}
}

@article{counterfactual-thinking,
  title={Counterfactual thinking.},
  author={Roese, Neal J},
  journal={Psychological bulletin},
  volume={121},
  number={1},
  pages={133},
  year={1997},
  publisher={American Psychological Association}
}

@article{counterfactual-explanations,
  title={Counterfactual explanations and how to find them: literature review and benchmarking},
  author={Guidotti, Riccardo},
  journal={Data Mining and Knowledge Discovery},
  pages={1--55},
  year={2022},
  publisher={Springer}
}

@article{NLP-explanation-survey,
    author = {Lyu, Qing and Apidianaki, Marianna and Callison-Burch, Chris},
    title = "{Towards Faithful Model Explanation in NLP: A Survey}",
    journal = {Computational Linguistics},
    pages = {1-67},
    year = {2024},
    month = {03},
    abstract = "{End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, that is an explanation should accurately represent the reasoning process behind the model’s prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00511},
    url = {https://doi.org/10.1162/coli\_a\_00511},
    eprint = {https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli\_a\_00511/2362278/coli\_a\_00511.pdf},
}

@article{NLP-posthoc-interpret-survey,
author = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
title = {Post-hoc Interpretability for Neural NLP: A Survey},
year = {2022},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3546577},
doi = {10.1145/3546577},
abstract = {Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {155},
numpages = {42},
keywords = {Interpretability, transparency, post-hoc explanations}
}

@article{GPT-2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{polyjuice,
  title={Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models},
  author={Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel S},
  journal={arXiv preprint arXiv:2101.00288},
  year={2021}
}

@misc{MiCE,
      title={Explaining NLP Models via Minimal Contrastive Editing (MiCE)}, 
      author={Alexis Ross and Ana Marasović and Matthew E. Peters},
      year={2021},
      eprint={2012.13985},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.13985}, 
}

@inproceedings{synthetisizing-Nguyen,
 author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Synthesizing the preferred inputs for neurons in neural networks via deep generator networks},
 volume = {29},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/5d79099fcdf499f12b79770834c0164a-Paper.pdf},
 year = {2016}
}

@inproceedings{synthetisizing-Barbalau,
  title={A generic and model-agnostic exemplar synthetization framework for explainable AI},
  author={Barbalau, Antonio and Cosma, Adrian and Ionescu, Radu Tudor and Popescu, Marius},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14--18, 2020, Proceedings, Part II},
  pages={190--205},
  year={2021},
  organization={Springer}
}

@InProceedings{Synthesizing-BlackBox-image-examplars,
author="Guidotti, Riccardo
and Monreale, Anna
and Matwin, Stan
and Pedreschi, Dino",
editor="Brefeld, Ulf
and Fromont, Elisa
and Hotho, Andreas
and Knobbe, Arno
and Maathuis, Marloes
and Robardet, C{\'e}line",
title="Black Box Explanation by Learning Image Exemplars in the Latent Feature Space",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="189--205",
abstract="We present an approach to explain the decisions of black box models for image classification. While using the black box to label images, our explanation method exploits the latent feature space learned through an adversarial autoencoder. The proposed method first generates exemplar images in the latent feature space and learns a decision tree classifier. Then, it selects and decodes exemplars respecting local decision rules. Finally, it visualizes them in a manner that shows to the user how the exemplars can be modified to either stay within their class, or to become counter-factuals by ``morphing'' into another class. Since we focus on black box decision systems for image classification, the explanation obtained from the exemplars also provides a saliency map highlighting the areas of the image that contribute to its classification, and areas of the image that push it into another class. We present the results of an experimental evaluation on three datasets and two black box models. Besides providing the most useful and interpretable explanations, we show that the proposed method outperforms existing explainers in terms of fidelity, relevance, coherence, and stability.",
isbn="978-3-030-46150-8"
}

@misc{what-is-interpretability,
      title={Towards A Rigorous Science of Interpretable Machine Learning}, 
      author={Finale Doshi-Velez and Been Kim},
      year={2017},
      eprint={1702.08608},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1702.08608}, 
}

@misc{UniversalTriggers,
      title={Universal Adversarial Triggers for Attacking and Analyzing NLP}, 
      author={Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
      year={2021},
      eprint={1908.07125},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{HotFlip,
      title={HotFlip: White-Box Adversarial Examples for Text Classification}, 
      author={Javid Ebrahimi and Anyi Rao and Daniel Lowd and Dejing Dou},
      year={2018},
      eprint={1712.06751},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{rubin1974estimating,
  title={Estimating causal effects of treatments in randomized and nonrandomized studies.},
  author={Rubin, Donald B},
  journal={Journal of educational Psychology},
  volume={66},
  number={5},
  pages={688},
  year={1974},
  publisher={American Psychological Association}
}

@article{holland1988causal,
  title={Causal inference, path analysis and recursive structural equations models},
  author={Holland, Paul W},
  journal={ETS Research Report Series},
  volume={1988},
  number={1},
  pages={i--50},
  year={1988},
  publisher={Wiley Online Library}
}

@article{Causal-inference-in-statistics,
author = {Judea Pearl},
title = {{Causal inference in statistics: An overview}},
volume = {3},
journal = {Statistics Surveys},
number = {none},
publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
pages = {96 -- 146},
keywords = {causal effects, causes of effects, confounding, counterfactuals, graphical methods, mediation, policy evaluation, potential-outcome, structural equation models},
year = {2009},
doi = {10.1214/09-SS057},
URL = {https://doi.org/10.1214/09-SS057}
}

@inproceedings{counterfactual-fairness,
 author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Fairness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{ChallengesApplyingExplainabilityMethods,
      title={Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models}, 
      author={Esma Balkir and Svetlana Kiritchenko and Isar Nejadgholi and Kathleen C. Fraser},
      year={2022},
      eprint={2206.03945},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.03945}, 
}

@misc{Scaling-laws-for-NN,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@inproceedings{WhyShouldITrustYou,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{AI-ethics,
  title={From what to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices},
  author={Morley, Jessica and Floridi, Luciano and Kinsey, Libby and Elhalal, Anat},
  journal={Science and engineering ethics},
  volume={26},
  number={4},
  pages={2141--2168},
  year={2020},
  publisher={Springer}
}

@misc{Oportunities-and-challenges-XAI,
      title={Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey}, 
      author={Arun Das and Paul Rad},
      year={2020},
      eprint={2006.11371},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2006.11371}, 
}

@article{black-box,
  title={Can we open the black box of AI?},
  author={Castelvecchi, Davide},
  journal={Nature News},
  volume={538},
  number={7623},
  pages={20},
  year={2016}
}

@inproceedings{OOD-1,
  title={Deep neural networks are easily fooled: High confidence predictions for unrecognizable images},
  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={427--436},
  year={2015}
}

@article{OOD-2,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@misc{OOD-multi-hipothesys,
  doi = {10.48550/ARXIV.2206.09522},
  url = {https://arxiv.org/abs/2206.09522},
  author = {Magesh, Akshayaa and Veeravalli, Venugopal V. and Roy, Anirban and Jha, Susmit},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Multiple Testing Framework for Out-of-Distribution Detection},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{XAI-the-new-42,
  title={Explainable AI: the new 42?},
  author={Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
  booktitle={International cross-domain conference for machine learning and knowledge extraction},
  pages={295--303},
  year={2018},
  organization={Springer}
}

@inproceedings{Counterfactuals-in-XAI,
  title={Counterfactuals in explainable artificial intelligence (XAI): Evidence from human reasoning.},
  author={Byrne, Ruth MJ},
  booktitle={IJCAI},
  pages={6276--6282},
  year={2019},
  organization={California, CA}
}

@article{XAI-insight-from-social-sciences,
title = {Explanation in artificial intelligence: Insights from the social sciences},
journal = {Artificial Intelligence},
volume = {267},
pages = {1-38},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
author = {Tim Miller},
keywords = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.}
}

@article{even-if-counterfactuals,
author = {Rachel McCloy and Ruth M.J. Byrne},
title = {Semifactual “even if” thinking},
journal = {Thinking \& Reasoning},
volume = {8},
number = {1},
pages = {41--67},
year = {2002},
publisher = {Routledge},
doi = {10.1080/13546780143000125},
URL = {https://doi.org/10.1080/13546780143000125},
eprint = {https://doi.org/10.1080/13546780143000125}
}

@book{hume1894enquiry,
  title={An enquiry concerning the human understanding: And an enquiry concerning the principles of morals},
  author={Hume, David},
  year={1894},
  publisher={Clarendon Press}
}

@book{josephson-josephson,
  title={Abductive inference: Computation, philosophy, technology},
  author={Josephson, John R and Josephson, Susan G},
  year={1996},
  publisher={Cambridge University Press}
}

@article{GDPR,
  title={Counterfactual explanations without opening the black box: Automated decisions and the GDPR},
  author={Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={841},
  year={2017},
  publisher={HeinOnline}
}

@inproceedings{PARE,
 author = {Ross, Alexis and Lakkaraju, Himabindu and Bastani, Osbert},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {18734--18746},
 publisher = {Curran Associates, Inc.},
 title = {Learning Models for Actionable Recourse},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/9b82909c30456ac902e14526e63081d4-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{alg-recourse-survey,
  author = {Amir{-}Hossein Karimi and Gilles Barthe and Bernhard Sch{\"{o}}lkopf and Isabel Valera},
  title = {A survey of algorithmic recourse: definitions, formulations, solutions, and prospects},
  journal = {CoRR},
  volume = {abs/2010.04050},
  year = {2020},
  url = {https://arxiv.org/abs/2010.04050},
  eprinttype = {arXiv},
  eprint = {2010.04050},
  timestamp = {Tue, 13 Oct 2020 15:25:23 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-2010-04050.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{AllenNLP,
  title={AllenNLP interpret: A framework for explaining predictions of NLP models},
  author={Wallace, Eric and Tuyls, Jens and Wang, Junlin and Subramanian, Sanjay and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:1909.09251},
  year={2019}
}

@inproceedings{measuring-unintended-bias,
author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
title = {Measuring and Mitigating Unintended Bias in Text Classification},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278729},
doi = {10.1145/3278721.3278729},
abstract = {We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {67–73},
numpages = {7},
keywords = {text classification, natural language processing, machine learning, fairness, algorithmic bias},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@article{beyond-algorithmic-bias,
  title={Moving beyond “algorithmic bias is a data problem”},
  author={Hooker, Sara},
  journal={Patterns},
  volume={2},
  number={4},
  year={2021},
  publisher={Elsevier}
}

@article{measures-of-fairness,
author = {Corbett-Davies, Sam and Gaebler, Johann D. and Nilforoshan, Hamed and Shroff, Ravi and Goel, Sharad},
title = {The measure and mismeasure of fairness},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {312},
numpages = {117},
keywords = {fair machine learning, consequentialism, discrimination}
}

@inproceedings{equality-of-opportunity,
 author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Equality of Opportunity in Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
 volume = {29},
 year = {2016}
}

@misc{demographic-parity,
      title={Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations}, 
      author={Alex Beutel and Jilin Chen and Zhe Zhao and Ed H. Chi},
      year={2017},
      eprint={1707.00075},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.00075}, 
}

@inproceedings{counterfactual-parity,
author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Counterfactual risk assessments, evaluation, and fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372851},
doi = {10.1145/3351095.3372851},
abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {582–593},
numpages = {12},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{counterfactual-equalized-odds,
author = {Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445902},
doi = {10.1145/3442188.3445902},
abstract = {In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed.The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria.In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {386–400},
numpages = {15},
keywords = {counterfactual, fairness, post-processing, risk assessment},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{PLL-scoring,
    title = "Masked Language Model Scoring",
    author = "Salazar, Julian  and
      Liang, Davis  and
      Nguyen, Toan Q.  and
      Kirchhoff, Katrin",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.240",
    doi = "10.18653/v1/2020.acl-main.240",
    pages = "2699--2712",
    abstract = "Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model{'}s WER by 30{\%} relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL{'}s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at \url{https://github.com/awslabs/mlm-scoring}.",
}

@article{PLL,
  author       = {Alex Wang and
                  Kyunghyun Cho},
  title        = {{BERT} has a Mouth, and It Must Speak: {BERT} as a Markov Random Field
                  Language Model},
  journal      = {CoRR},
  volume       = {abs/1902.04094},
  year         = {2019},
  url          = {http://arxiv.org/abs/1902.04094},
  eprinttype    = {arXiv},
  eprint       = {1902.04094},
  timestamp    = {Tue, 21 May 2019 18:03:40 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1902-04094.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{log_likelihood-for-acceptability,
  title={Grammaticality, acceptability, and probability: A probabilistic view of linguistic knowledge},
  author={Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
  journal={Cognitive science},
  volume={41},
  number={5},
  pages={1202--1241},
  year={2017},
  publisher={Wiley Online Library}
}

@inproceedings{SHAP,
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{counterfactual-fairness-text-clf,
author = {Garg, Sahaj and Perot, Vincent and Limtiaco, Nicole and Taly, Ankur and Chi, Ed H. and Beutel, Alex},
title = {Counterfactual Fairness in Text Classification through Robustness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3317950},
doi = {10.1145/3306618.3317950},
abstract = {In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that "Some people are gay" is toxic while "Some people are straight" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {219–226},
numpages = {8},
keywords = {counterfactual fairness, fairness, robustness, text classification},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@misc{equalizing-recourse,
      title={Equalizing Recourse across Groups}, 
      author={Vivek Gupta and Pegah Nokhiz and Chitradeep Dutta Roy and Suresh Venkatasubramanian},
      year={2019},
      eprint={1909.03166},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1909.03166}, 
}

@article{recourse-fairness,
title={On the Fairness of Causal Algorithmic Recourse}, 
volume={36},
url={https://ojs.aaai.org/index.php/AAAI/article/view/21192},
DOI={10.1609/aaai.v36i9.21192},
abstractNote={Algorithmic fairness is typically studied from the perspective of predictions. Instead, here we investigate fairness from the perspective of recourse actions suggested to individuals to remedy an unfavourable classification. We propose two new fair-ness criteria at the group and individual level, which—unlike prior work on equalising the average group-wise distance from the decision boundary—explicitly account for causal relationships between features, thereby capturing downstream effects of recourse actions performed in the physical world. We explore how our criteria relate to others, such as counterfactual fairness, and show that fairness of recourse is complementary to fairness of prediction. We study theoretically and empirically how to enforce fair causal recourse by altering the classifier and perform a case study on the Adult dataset. Finally, we discuss whether fairness violations in the data generating process revealed by our criteria may be better addressed by societal interventions as opposed to constraints on the classifier.},
number={9},
journal={Proceedings of the AAAI Conference on Artificial Intelligence},
author={Kügelgen, Julius von and Karimi, Amir-Hossein and Bhatt, Umang and Valera, Isabel and Weller, Adrian and Schölkopf, Bernhard},
year={2022},
month={Jun.},
pages={9584-9594}
}

@inproceedings{xai-through-diverse-counterfactuals,
author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
title = {Explaining machine learning classifiers through diverse counterfactual explanations},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372850},
doi = {10.1145/3351095.3372850},
abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {607–617},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{counterfactual-xai-review,
  title={Counterfactual explanations for machine learning: A review},
  author={Verma, Sahil and Dickerson, John and Hines, Keegan},
  journal={arXiv preprint arXiv:2010.10596},
  volume={2},
  pages={1},
  year={2020}
}

@inproceedings{pertinent-negatives,
 author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{algorithmic-recourse-1,
      title={Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems}, 
      author={Shalmali Joshi and Oluwasanmi Koyejo and Warut Vijitbenjaronk and Been Kim and Joydeep Ghosh},
      year={2019},
      eprint={1907.09615},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.09615}, 
}

@inproceedings{algorithmic-recourse-2,
author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
title = {Actionable Recourse in Linear Classification},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287566},
doi = {10.1145/3287560.3287566},
abstract = {Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood.In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {10–19},
numpages = {10},
keywords = {recourse, integer programming, credit scoring, classification, audit, accountability},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@book{Weapons-of-math-destruction,
  title={Weapons of math destruction: How big data increases inequality and threatens democracy},
  author={O'neil, Cathy},
  year={2017},
  publisher={Crown}
}

@misc{industrial-defense-of-the-algorithm,
  title={Seeing the sort: The aesthetic and industrial defense of “the algorithm”},
  author={Sandvig, Christian},
  year={2014}
}

@article{opacity-in-ML,
    author = {Jenna Burrell},
    title ={How the machine ''thinks'': Understanding opacity in machine learning algorithms},
    journal = {Big Data \& Society},
    volume = {3},
    number = {1},
    pages = {2053951715622512},
    year = {2016},
    doi = {10.1177/2053951715622512},
    URL = {https://doi.org/10.1177/2053951715622512},
    eprint = {https://doi.org/10.1177/2053951715622512},
    abstract = { This article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. These mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. In this article, I draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy, (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. The analysis in this article gets inside the algorithms themselves. I cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. I argue that recognizing the distinct forms of opacity that may be coming into play in a given application is a key to determining which of a variety of technical and non-technical solutions could help to prevent harm. }
}

@misc{WinoBias,
      title={Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods}, 
      author={Jieyu Zhao and Tianlu Wang and Mark Yatskar and Vicente Ordonez and Kai-Wei Chang},
      year={2018},
      eprint={1804.06876},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.06876}, 
}

@inproceedings{USE,
    title = "Universal Sentence Encoder for {E}nglish",
    author = "Cer, Daniel  and
      Yang, Yinfei  and
      Kong, Sheng-yi  and
      Hua, Nan  and
      Limtiaco, Nicole  and
      St. John, Rhomni  and
      Constant, Noah  and
      Guajardo-Cespedes, Mario  and
      Yuan, Steve  and
      Tar, Chris  and
      Strope, Brian  and
      Kurzweil, Ray",
    editor = "Blanco, Eduardo  and
      Lu, Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2029",
    doi = "10.18653/v1/D18-2029",
    pages = "169--174",
    abstract = "We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.",
}

@misc{Detoxify,
  title={Detoxify},
  author={Hanu, Laura and {Unitary team}},
  howpublished={Github. https://github.com/unitaryai/detoxify},
  year={2020}
}

@article{civil-comments,
author    = {Daniel Borkan and
              Lucas Dixon and
              Jeffrey Sorensen and
              Nithum Thain and
              Lucy Vasserman},
title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text
              Classification},
journal   = {CoRR},
volume    = {abs/1903.04561},
year      = {2019},
url       = {http://arxiv.org/abs/1903.04561},
archivePrefix = {arXiv},
eprint    = {1903.04561},
timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{toxic-wikipedia,
    author = {cjadams and Jeffrey Sorensen and Julia Elliott and Lucas Dixon and Mark McDonald and nithum and Will Cukierski},
    title = {Toxic Comment Classification Challenge},
    year = {2017},
    howpublished = {\url{https://kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge}},
    note = {Kaggle}
}

@inproceedings{auto-debias,
    title = "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
    author = "Guo, Yue  and
      Yang, Yi  and
      Abbasi, Ahmed",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.72",
    doi = "10.18653/v1/2022.acl-long.72",
    pages = "1012--1023",
    abstract = "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for \textit{biased prompts} such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed \textbf{Auto-Debias} approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models{'} understanding abilities, as shown using the GLUE benchmark.",
}

@misc{gender-stereotype-word-list,
      title={Debiasing Pre-trained Contextualised Embeddings}, 
      author={Masahiro Kaneko and Danushka Bollegala},
      year={2021},
      eprint={2101.09523},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.09523}, 
}

@ONLINE{wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
}

@misc{NLP-counterfactual-survey,
      title={A Natural Language Counterfactual Generation}, 
      author={Yongjie Wang and Xiaoqi Qiu and Yu Yue and Xu Guo and Zhiwei Zeng and Yuhong Feng and Zhiqi Shen},
      year={2024},
      eprint={2407.03993},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.03993}, 
}

@article{GYC,
  title={Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text},
  volume={35},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/17594},
  DOI={10.1609/aaai.v35i15.17594},
  abstractNote={Machine Learning has seen tremendous growth recently, which has led to a larger adaptation of ML systems for educational assessments, credit risk, healthcare, employment, criminal justice, to name a few. The trustworthiness of ML and NLP systems is a crucial aspect and requires a guarantee that the decisions they make are fair and robust. Aligned with this, we propose a novel framework GYC, to generate a set of exhaustive counterfactual text, which are crucial for testing these ML systems. Our main contributions include a) We introduce GYC, a framework to generate counterfactual samples such that the generation is plausible, diverse, goal-oriented, and effective, b) We generate counterfactual samples, that can direct the generation towards a corresponding \texttt{condition} such as named-entity tag, semantic role label, or sentiment. Our experimental results on various domains show that GYC generates counterfactual text samples exhibiting the above four properties. GYC generates counterfactuals that can act as test cases to evaluate a model and any text debiasing algorithm.},
  number={15},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Madaan, Nishtha and Padhi, Inkit and Panwar, Naveen and Saha, Diptikalyan},
  year={2021},
  month={May},
  pages={13516-13524}
}

@ARTICLE{fuzzy-logic,
  author={Zadeh, L.A.},
  journal={Computer}, 
  title={Fuzzy logic}, 
  year={1988},
  volume={21},
  number={4},
  pages={83-93},
  keywords={Fuzzy logic;Decision making;Linguistics;Application software;Process control;Fuzzy sets;Expert systems;Uncertainty;Precision engineering},
  doi={10.1109/2.53}
}

@article{BERT-gender-bias-1,
  title={Detecting gender bias in transformer-based models: A case study on bert},
  author={Li, Bingbing and Peng, Hongwu and Sainju, Rajat and Yang, Junhuan and Yang, Lei and Liang, Yueying and Jiang, Weiwen and Wang, Binghui and Liu, Hang and Ding, Caiwen},
  journal={arXiv preprint arXiv:2110.15733},
  year={2021}
}

@article{BERT-gender-bias-2,
  title={Investigating gender bias in bert},
  author={Bhardwaj, Rishabh and Majumder, Navonil and Poria, Soujanya},
  journal={Cognitive Computation},
  volume={13},
  number={4},
  pages={1008--1018},
  year={2021},
  publisher={Springer}
}

@inproceedings{BERT-gender-bias-3,
   title={Gender Bias in BERT - Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task},
   url={http://dx.doi.org/10.18653/v1/2022.gebnlp-1.20},
   DOI={10.18653/v1/2022.gebnlp-1.20},
   booktitle={Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)},
   publisher={Association for Computational Linguistics},
   author={Jentzsch, Sophie and Turan, Cigdem},
   year={2022},
   pages={184–199}
}

@misc{scene,
      title={SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals}, 
      author={Haoran Zheng and Utku Pamuksuz},
      year={2024},
      eprint={2408.04575},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.04575}, 
}

@inproceedings{counterfactual_fairness,
 author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Fairness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{algorithmic_recourse_1,
      title={Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems}, 
      author={Shalmali Joshi and Oluwasanmi Koyejo and Warut Vijitbenjaronk and Been Kim and Joydeep Ghosh},
      year={2019},
      eprint={1907.09615},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.09615}, 
}

@inproceedings{algorithmic_recourse_2,
author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
title = {Actionable Recourse in Linear Classification},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287566},
doi = {10.1145/3287560.3287566},
abstract = {Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood.In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {10–19},
numpages = {10},
keywords = {recourse, integer programming, credit scoring, classification, audit, accountability},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@misc{reverse-inference,
  title={Causality and statistical learning},
  author={Gelman, Andrew},
  year={2011},
  publisher={University of Chicago Press Chicago, IL}
}

@inproceedings{man-is-to-computer,
 author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume = {29},
 year = {2016}
}

@article{secret-life-of-pronouns,
title = {The secret life of pronouns},
journal = {New Scientist},
volume = {211},
number = {2828},
pages = {42-45},
year = {2011},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(11)62167-2},
url = {https://www.sciencedirect.com/science/article/pii/S0262407911621672},
author = {James W. Pennebaker},
abstract = {The smallest words in our vocabulary often reveal the most about us, says James W. Pennebaker, including our levels of honesty and thinking style}
}

@inproceedings{geo-lexical-variation,
  title={A latent variable model for geographic lexical variation},
  author={Eisenstein, Jacob and O’Connor, Brendan and Smith, Noah A and Xing, Eric},
  booktitle={Proceedings of the 2010 conference on empirical methods in natural language processing},
  pages={1277--1287},
  year={2010}
}

@article{social-media-language,
  title={Gaining insights from social media language: Methodologies and challenges.},
  author={Kern, Margaret L and Park, Gregory and Eichstaedt, Johannes C and Schwartz, H Andrew and Sap, Maarten and Smith, Laura K and Ungar, Lyle H},
  journal={Psychological methods},
  volume={21},
  number={4},
  pages={507},
  year={2016},
  publisher={American Psychological Association}
}

@inproceedings{critical-survey-on-bias,
    title = "Language (Technology) is Power: A Critical Survey of ``Bias'' in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.485/",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing ``bias'' in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing ``bias'' is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating ``bias'' are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing ``bias'' in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of ``bias''{---}i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities."
}

@article{quantifying-social-bias,
    author = {Czarnowska, Paula and Vyas, Yogarshi and Shah, Kashif},
    title = {Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1249-1267},
    year = {2021},
    month = {11},
    abstract = {Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model’s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00425},
    url = {https://doi.org/10.1162/tacl_a_00425},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00425/1972677/tacl_a_00425.pdf},
}

@inproceedings{sociodemographic-bias,
    title = "Sociodemographic Bias in Language Models: A Survey and Forward Path",
    author = "Gupta, Vipul  and
      Narayanan Venkit, Pranav  and
      Wilson, Shomir  and
      Passonneau, Rebecca",
    editor = "Fale{\'n}ska, Agnieszka  and
      Basta, Christine  and
      Costa-juss{\`a}, Marta  and
      Goldfarb-Tarrant, Seraphina  and
      Nozza, Debora",
    booktitle = "Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.gebnlp-1.19/",
    doi = "10.18653/v1/2024.gebnlp-1.19",
    pages = "295--322",
    abstract = "Sociodemographic bias in language models (LMs) has the potential for harm when deployed in real-world settings. This paper presents a comprehensive survey of the past decade of research on sociodemographic bias in LMs, organized into a typology that facilitates examining the different aims: types of bias, quantifying bias, and debiasing techniques. We track the evolution of the latter two questions, then identify current trends and their limitations, as well as emerging techniques. To guide future research towards more effective and reliable solutions, and to help authors situate their work within this broad landscape, we conclude with a checklist of open questions."
}

@misc{cultural-awareness-in-LM,
      title={Survey of Cultural Awareness in Language Models: Text and Beyond}, 
      author={Siddhesh Pawar and Junyeong Park and Jiho Jin and Arnav Arora and Junho Myung and Srishti Yadav and Faiz Ghifari Haznitrama and Inhwa Song and Alice Oh and Isabelle Augenstein},
      year={2024},
      eprint={2411.00860},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.00860}, 
}

@article{bias-and-fairness-in-ML,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {115},
numpages = {35},
keywords = {representation learning, natural language processing, machine learning, deep learning, Fairness and bias in artificial intelligence}
}

@misc{RIPA,
      title={Understanding Undesirable Word Embedding Associations}, 
      author={Kawin Ethayarajh and David Duvenaud and Graeme Hirst},
      year={2019},
      eprint={1908.06361},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.06361}, 
}

@inproceedings{CEAT,
author = {Guo, Wei and Caliskan, Aylin},
title = {Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462536},
doi = {10.1145/3461702.3462536},
abstract = {With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that occur in contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models.Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6\% and 82.7\%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3\% and 13.3\%. EIBD reaches an accuracy of 84.7\% and 65.3\%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2\% and 6.1\%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {122–133},
numpages = {12},
keywords = {word embeddings, social psychology, language models, intersectionality, bias, AI ethics},
location = {Virtual Event, USA},
series = {AIES '21}
}

@misc{debiasing-makes-bias-accessible,
      title={Debiasing Methods in Natural Language Understanding Make Bias More Accessible}, 
      author={Michael Mendelson and Yonatan Belinkov},
      year={2021},
      eprint={2109.04095},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.04095}, 
}

@misc{debias-via-counterfactual,
      title={Reducing Sentiment Bias in Language Models via Counterfactual Evaluation}, 
      author={Po-Sen Huang and Huan Zhang and Ray Jiang and Robert Stanforth and Johannes Welbl and Jack Rae and Vishal Maini and Dani Yogatama and Pushmeet Kohli},
      year={2020},
      eprint={1911.03064},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.03064}, 
}

@incollection{machine-bias-book,
  title={Machine bias},
  author={Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  booktitle={Ethics of data and analytics},
  pages={254--264},
  year={2022},
  publisher={Auerbach Publications}
}

@inproceedings{NLP-risk-child-protective-service,
author = {Field, Anjalie and Coston, Amanda and Gandhi, Nupoor and Chouldechova, Alexandra and Putnam-Hornstein, Emily and Steier, David and Tsvetkov, Yulia},
title = {Examining risks of racial biases in NLP tools for child protective services},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594094},
doi = {10.1145/3593013.3594094},
abstract = {Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1479–1492},
numpages = {14},
keywords = {CPS, NLP, bias, child protection system, race, text processing},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@misc{lipstick,
      title={Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them}, 
      author={Hila Gonen and Yoav Goldberg},
      year={2019},
      eprint={1903.03862},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1903.03862}, 
}

@inproceedings{intrinsic-not-correlate,
    title = "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    author = "Goldfarb-Tarrant, Seraphina  and
      Marchant, Rebecca  and
      Mu{\~n}oz S{\'a}nchez, Ricardo  and
      Pandya, Mugdha  and
      Lopez, Adam",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.150/",
    doi = "10.18653/v1/2021.acl-long.150",
    pages = "1926--1940",
    abstract = "Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech."
}

@article{gender-resume-differences,
author = {Qu, Qian  and Liu, Quan-Hui  and Gao, Jian  and Huang, Shudong  and Feng, Wentao  and Yue, Zhongtao  and Lu, Xin  and Zhou, Tao  and Lv, Jiancheng },
title = {Gender differences in resume language and gender gaps in salary expectations},
journal = {Journal of The Royal Society Interface},
volume = {22},
number = {227},
pages = {20240784},
year = {2025},
doi = {10.1098/rsif.2024.0784},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2024.0784},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2024.0784}
,
    abstract = { How men and women present themselves in their resumes may affect their opportunity in job seeking. To investigate gender differences in resume writing and how they are associated with gender gaps in the labour market, we analysed 6.9 million resumes of Chinese job applicants in this study. Results reveal substantial gender resume differences, where women and men show distinct patterns in both simple language features and high-level semantic structures in the word embedding space of resumes. In particular, women tend to use shorter resumes, longer sentences and a more diverse set of unique words. Neural network models trained on resumes can predict gender with 80\% accuracy, and the accuracy decreases with education levels and text standardization requirements. Moreover, while better language skills are associated with higher salary expectations, this positive relationship is magnified for men but weakened for women in women-dominated occupations. This study presents a new venue for the understanding of gender differences and provides empirical findings on how men and women are different in self-portraying and job seeking. }
}

@inproceedings{embedding-relations-1,
  title={Linguistic regularities in continuous space word representations},
  author={Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey},
  booktitle={Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies},
  pages={746--751},
  year={2013}
}

@article{embedding-relations-2,
author = {Rubenstein, Herbert and Goodenough, John B.},
title = {Contextual correlates of synonymy},
year = {1965},
issue_date = {Oct. 1965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/365628.365657},
doi = {10.1145/365628.365657},
journal = {Commun. ACM},
month = oct,
pages = {627–633},
numpages = {7}
}

@InProceedings{R-LACE,
  title = 	 {Linear Adversarial Concept Erasure},
  author =       {Ravfogel, Shauli and Twiton, Michael and Goldberg, Yoav and Cotterell, Ryan D},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {18400--18421},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/ravfogel22a/ravfogel22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/ravfogel22a.html},
  abstract = 	 {Modern neural models trained on textual data rely on pre-trained representations that emerge without direct supervision. As these representations are increasingly being used in real-world applications, the inability to <em>control</em> their content becomes an increasingly important problem. In this work, we formulate the problem of identifying a linear subspace that corresponds to a given concept, and removing it from the representation. We formulate this problem as a constrained, linear minimax game, and show that existing solutions are generally not optimal for this task. We derive a closed-form solution for certain objectives, and propose a convex relaxation that works well for others. When evaluated in the context of binary gender removal, the method recovers a low-dimensional subspace whose removal mitigates bias by intrinsic and extrinsic evaluation. Surprisingly, we show that the method—despite being linear—is highly expressive, effectively mitigating bias in the output layers of deep, nonlinear classifiers while maintaining tractability and interpretability.}
}

@misc{circuit-breaking,
      title={Circuit Breaking: Removing Model Behaviors with Targeted Ablation}, 
      author={Maximilian Li and Xander Davies and Max Nadeau},
      year={2024},
      eprint={2309.05973},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.05973}, 
}

@misc{geometry-of-truth,
      title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets}, 
      author={Samuel Marks and Max Tegmark},
      year={2024},
      eprint={2310.06824},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.06824}, 
}

@misc{gradient-attribution,
      title={Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}, 
      author={Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
      year={2014},
      eprint={1312.6034},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1312.6034}, 
}

@article{beam-search,
title = {Speech understanding systems: Report of a steering committee},
journal = {Artificial Intelligence},
volume = {9},
number = {3},
pages = {307-316},
year = {1977},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(77)90026-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370277900261},
author = {M.F. Medress and F.S. Cooper and J.W. Forgie and C.C. Green and D.H. Klatt and M.H. O'Malley and E.P. Neuburg and A. Newell and D.R. Reddy and B. Ritea and J.E. Shoup-Hummel and D.E. Walker and W.A. Woods},
abstract = {A five-year interdisciplinary effort by speech scientists and computer scientists has demonstrated the feasibility of programming a computer system to “understand” connected speech, i.e., translate it into operational form and respond accordingly. An operational system (HARPY) accepts speech from five speakers, interprets a 1000-word vocabulary, and attains 91 percent sentence accuracy. This Steering Committee summary report describes the project history, problem, goals, and results.}
}

@inproceedings{kernelized-concept-erasure,
    title = "Adversarial Concept Erasure in Kernel Space",
    author = "Ravfogel, Shauli  and
      Vargas, Francisco  and
      Goldberg, Yoav  and
      Cotterell, Ryan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.405/",
    doi = "10.18653/v1/2022.emnlp-main.405",
    pages = "6034--6055",
    abstract = "The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how human-interpretable concepts, such as gender, are encoded in these representations would improve the ability of users to control the content of these representations and analyze the working of the models that rely on them. One prominent approach to the control problem is the identification and removal of linear concept subspaces {--} subspaces in the representation space that correspond to a given concept. While those are tractable and interpretable, neural network do not necessarily represent concepts in linear subspaces. We propose a kernelization of the recently-proposed linear concept-removal objective, and show that it is effective in guarding against the ability of certain nonlinear adversaries to recover the concept. Interestingly, our findings suggest that the division between linear and nonlinear models is overly simplistic: when considering the concept of binary gender and its neutralization, we do not find a single kernel space that exclusively contains all the concept-related information. It is therefore challenging to protect against all nonlinear adversaries at once."
}

@inbook{indirect-effect,
author = {Pearl, Judea},
title = {Direct and Indirect Effects},
year = {2022},
isbn = {9781450395861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3501714.3501736},
booktitle = {Probabilistic and Causal Inference: The Works of Judea Pearl},
pages = {373–392},
numpages = {20}
}
