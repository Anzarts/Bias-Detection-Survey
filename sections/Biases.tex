\section{Background}
\label{sec:background}

In this section we review some basic concepts necessary for a better discussion of Prediction Bias Analysis. In Section \ref{sec:backgroun:bias} we expand the definition of Prediction Bias with the concept of unintended bias, which is the subcategory of potentially harmful biases. %In Section \ref{sec:backgroun:indirect-effect} we review the concepts of direct and indirect effect from the causal infenrence literature, which quantify the extent to which a variable if influenced by another. In the context of Prediction Bias Analysis, they correspond to two complementary assessments, necessary to identify Prediction Bias.

\subsection{Prediction Bias in NLP}
\label{sec:backgroun:bias}

Despite of the large number of works addressing bias in NLP, there is a lack of consensus regarding the definition of bias~\cite{critical-survey-on-bias}. The discussion over the various definitions of bias constitutes a complex subject that will not be thoroughly addressed in this survey. In this work, we will restrict our analysis to the definitions outlined in Section \ref{sec:intro:social_and_prediction_bias}, where Social Bias corresponds to a bias present in human prejudices and Prediction Bias to a bias present in the model's computations. Prediction Bias in NLP has been described previous works as the prior that informs a model's predictions~\cite{fairness-survey,predictive-bias-NLP}. In essence, Prediction Bias can be understood as the associations between features of the input and the output of the model. According to this definition, all models have Prediction Bias, however, it is not something inherently ploblematic, but another gear in the model's mechanism.

Biases can be harmful when they come from harmful precedents~\cite{WEAT}. Biases that are not aligned with reality, or are aligned with a reality that we do not wish the model to learn from, are denominated as unintended biases~\cite{fairness-survey,predictive-bias-NLP}. This would be the case for the association between qualification for the job and a feature that correlates with gender in the hiring model example.

The majority of predictive models in NLP are trained with real-world text samples, which are unavoidably biased by the context in which they are written and the demographic of who writes them~\cite{geo-lexical-variation,social-media-language,secret-life-of-pronouns}. In consequence, there is a high chance that the models replicate those biases. This can lead models to pick up patterns that do not generalize to other contexts or demographics, or rely on undesired relations, resulting in unfair or harmful predictions~\cite{machine-bias-book,bias-and-fairness-in-ML,Weapons-of-math-destruction,predictive-bias-NLP}. Even if the data does not present undesired biases, models themselves may still manifest unintended biased behavior due to certain design choices~\cite{bias-and-fairness-in-ML} or by inheriting them from biased representations~\cite{man-is-to-computer,WEAT}.

Unintended biases, for predictive model in NLP, can be divided into four categories according to the source of the bias~\cite{predictive-bias-NLP}:

\begin{itemize}
    \item \textbf{Semantic Bias:} Emerges when the word-embeddings used by the model encode biased relations. \textit{The information represented in the embeddings include a Social Bias (e.g., gender in genderless words).}
    \item \textbf{Label Bias:} Emerges when the model learns predictions that diverge substantially from the ideal distribution, product of labels aligned with a (not desired) biased reality. \textit{The training data is affected by a Social Bias, as in in the hiring model example (}\textsl{Ex1}\textit{)}.
    \item \textbf{Selection Bias:} Emerges when the model learns from data that is non-representative of the distribution to where it would be applied. \textit{The training data is selection is affected by a Social Bias (e.g., use mostly texts written by middle-
aged white men).}
    \item \textbf{Overamplification:} Emerges when the model itself pick up small difference in the data, and amplify them to be much larger in the predicted outcomes. \textit{The model develops its own unintended Prediction Bias, without the influence of a Social Bias.}
\end{itemize}

As indicated in the text in cursive, these categories of origin of unintended bias correspond to different forms of how Social Bias can cause Prediction Bias. The identification of the origin of bias can help in the development of unbiased models. Nevertheless, it requieres analysis beyond the boundaries of Prediction Bias Analysis, so it will not be addressed in this work.

