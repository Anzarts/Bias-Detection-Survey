\section{Introduction}
\label{sec:intro}

Alongside the increase in capacity and influence of algorithms, there is an increase in the concerns and risks of them inadvertedly perpetuating human biases~\cite{measures-of-fairness,Weapons-of-math-destruction}. This phenomenon is particularly evident in the context of neural networks developed for Natural Language Processing (NLP), as they learn directly from human-generated texts. The delegation of the decision-making process to these algorithms has the potential to engender negative societal impact if there are undetected or neglected biases~\cite{critical-survey-on-bias,NLP-risk-child-protective-service,sociodemographic-bias,bias-and-fairness-in-ML,predictive-bias-NLP}.

In this survey, we present a brief review of bias evaluation and explainability methods, and discuss them through the scope of prediction bias analysis. We will use the term predictive model (in NLP), to refer to any model that takes text as input, and produces a prediction, decision, or classification as output (e.g., toxic text classification or spam filtering). In the same line, an explainability method would be any method designed to provide insight regarding the prediction process, such as which variables are relevant or what information is being processed at a specific layer of the model.

Before deliniating the concept of prediction bias analysis, to ilustrate the risks of unassessed biases in predictive models in NLP, consider the following toy example:

\begin{displayquote}
\textbf{\textsc{Ex1}} A certain company is accused of favoring men over women in their hiring process. To solve this problem, the company decides to leave the process to a neural network. This network reads an applicant's anonimized resume, and determines if they should be hired or not. The network is trained with the data from the previous processes, so it replicates the same biases. The next time the company is questioned about its hiring process, they respond that it is managed by an algorithm and, unlike humans, algorithms are objective, so if more men than women are hired, it must be because there are more men who are better qualified for the job.
\end{displayquote}

In this example, the bias is the association of gender with qualification for the job, and it is originated by the use of biased training data. A proper evaluation of bias is neglected, and an undesired bias is perpetuated, or even worsened, under the illusion of ``objective'' prediction.

%In Section \ref{sec:bias} we provide a more precise definition of bias in NLP and its origins.

\subsection{Social Bias and Predicion Bias}
\label{sec:intro:social_and_prediction_bias}

A substantial corpus of research has been dedicated to the examination of bias in predictibe models in NLP, and in NLP algorithms in general. Most of them can be encapsuled in three categories: characterization of bias and its risks, measuring bias, and debiasing. The last two categories are closely intertwined, given that debiasing methods, which aim to remove a specific bias, often do so by reducing the metrics defined in the bias measuring literature. However, it has been observed that the metric reduction approach is more likely an elimination of symptoms rather than biases~\cite{fairness-survey,lipstick}.

In general, bias measuring works choose a bias, such as gender or racial bias, and propose a metric to quantify the presence of this bias in either the model representations or responses. These are referred in the literature as intrinsic and extrinsic metrics, respectively~\cite{quantifying-social-bias,sociodemographic-bias}. Intrinsic metrics have been found to not correlate with the responses of the model~\cite{intrinsic-not-correlate}, while extrinsic metrics are a subcategory fairness metrics~\cite{measures-of-fairness}, which, in turn, are metrics directly designed to measure the consequences and symptoms of biases (or other problematic elements), rather than the bias itself.

% ----------------------------

We argue that these issues in the analysis of bias may come from an indiscriminate use of the term bias, to refer to two different terms: \textit{Social Bias} and \textit{Prediction Bias} (Figure \ref{fig:bias-diagram}). To better explain this idea, we will continue with the hiring model example:

\begin{displayquote}
\textbf{\textsc{Ex2}} An organization against gender bias, dertermined to demonstrate that the model used by the company is biased, recolect the resumes of 100 applicants and their results. They find that men were accepted 4 times more than women, even when they had similar background. With this fairness test, the organization is sure that the model favors men over women, however, the company points out a key detail: the resumes are anonimized. The model does not have any information regarding gender, and thus cannot be biased. The organization examinates the applications again, this time focusing on the diference between accepted and rejected resumes. They find that the rejected resumes tend to use longer sentences with more unique words, and that this characteristic is more frequent in women's resumes\footnote{This toy example is based on the findings of Qu et al.\cite{gender-resume-differences}}. After editing the resumes to have a similar style and passing them to the model, it is found that the rejection rate is now equitative between men and women, probing that it was, in fact, biased.
\end{displayquote}

We have previously stated that, in the example, the bias is the association between gender and qualification for the job. To be precise, this is the Social Bias being replicated by the model. Here, even if a metric indicates that the model's answers are biased, it does not explain why it happens, because it is only measuring the Social Bias in its responses. Only after examining a set of input-output samples, it is found a correlation between a single attribute of the input and a particular output, an association that can be called a Prediction Bias.

\begin{paracol}{2}

We define Social Bias and Prediction Biases as follows:

\begin{itemize}
    \item\textbf{Social Bias:} Prejudices and associations, related to social groups, made by humans. This would be gender discrimination in the example.
    \item\textbf{Prediction Bias:} Association between variables encoded in the operations of the model. This would be the correlation between the variables sentence lenght and word diversity and the outcome in the example.
\end{itemize}

Social Bias can be a cuase and a consequence of Prediction Bias, but a measurement of Social Bias alone is not enough to correct Prediction Bias. %We provide further discussion and previously definitions of the concept of prediction bias in Section \ref{sec:backgroun:bias}.

\switchcolumn

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Imgs/bias_diagram.drawio.pdf}
    \caption{Use of the term ``bias'' in bias measurement and explainability literature.}
    \label{fig:bias-diagram}
\end{figure}

\end{paracol}

\subsection{Prediction Bias Analysis}
\label{sec:intro:prediction_bias_analysis}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Imgs/concepts_bias.drawio.pdf}
    \caption{...}
    \label{fig:concepts}
\end{figure}

We indentify an alternative approach for addressing the analysis of bias in predictive models, which could lead to better debiasing methods, in the distinction of Social Bias and Prediction Bias. Rather than measuring the extent to which Social Bias is expressed by a model, we endeavor to identify the Prediction Bias, i.e., the biased associations encoded in the model's prediction mechanism.

In general, we want to reduce a Social Bias expressed by the model, which is caused by a Prediction Bias. However, even if this same Prediction Bias is learned by the model due to the presence of the Social Bias, they are not equivalent. Only proving or measuring the presence of Social Bias in the model's responses does not give enough information regarding the Prediction Bias, and can led to unaccurate assesments or insufficient debiasing. Consider the following alternative version of the example:

\begin{displayquote}
    \textbf{\textsc{Ex3}} The company, aware that their model will be accused of perpetuating gender bias, decides to include the gender of the applicants in the input and train the model following a debiasing procedure. The procedure consist on duplicating each resume in the training data, varying only the gender, in order to prevent the model from learning gender bias. Once the training is finished, they test the model and find that gender has no correlation with the output, so the company decides that it is safe to use. Some months later, the company is denounced by the organization, allegating that the model perpetuates gender bias.
\end{displayquote}

In this case, Social Bias and Prediction Bias are confounded. The debiasing method and bias analysis are aimed toward the declared applicant's gender, assuming that the model would learn an explicit association between gender and qualification. However, the model reproduces the bias through a mediator variable\footnote{This concept is explicated in more detail in Section \ref{sec:backgroun:indirect-effect}}. A complete analysis of prediction bias is neglected, failing identify the actual association that cuase the model to expressed social bias.

%In the example, the association is identified through manual examination. However, it would be preferable to have a mechanism capable of automatically detecting bias. 

We denominate the process of indentifying and proving associations in the model, i.e. the exact Prediction Bias, as Prediction Bias Analysis. Even though prediction bias is relevant as already proposed, methods for explicitly addressing Prediction Bias Analysis are scarce. Despite this, we identify an abundance of related methods that, even though some were not specifically designed for this bias evaluation, could be readily adapted for it.

This survey's scope is to structure these works to present them in a unified way, focused around their applicability for the purpouse of Prediction Bias Analysis. In particular, we review and discuss the applicability in Prediction Bias Analysis of methods across 5 explainability paradigms: (1) embedding association, (2) concept erasure, (3) ablation, (4) counterfactual examples, and (5) activation maximiztion. Additionally, we discuss the concepts of direct and indirect effect, which we found crucial to understand prediction bias analysis, and that can serve to further improve future methods in the field. 

We categorize the methods into 2 groups regarding how they work, and into 2 groups regarding the previously information they require about the variables. The first groups are: (I) Examination methods, that examinates the representations used or generated by the model or its operations; and (II) example generation methods, that generates examples that might show the biases of the model. The other 2 groups are: (III) Confirmatory methods, that verify if there is bias associated to a known variable; and (IV) Exploratory methods, that can found previously unknown variables related a bias. Figure \ref{fig:concepts} summarizes the definitions of Prediction Bias Analysis and these four groups.

The remaining of this document is structured as follows: in Section \ref{sec:backgroun:bias} we provide further discussion and previously definitions of the concept of Prediction Bias; in Section \ref{sec:backgroun:indirect-effect} we present and discuss the concepts of direct and indirect effect; in Section \ref{sec:examination} we review the Examination methods; in Section \ref{sec:examples} we review the Example Generation methods; and in Section \ref{sec:discussion} \textcolor{red}{[...]}. %We 

% The scope of the survey is restricted to methods that can be applied on predictive models with transformer architecture.

\begin{table}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|l|cc|cc|}
        \hline
         & \multicolumn{1}{c|}{Examination} & \multicolumn{1}{c|}{Example Generation} & \multicolumn{1}{c|}{Confirmatory} & \multicolumn{1}{c|}{Exploratory} \\ \hline
        WEAT & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        SEAT & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        CEAT & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        RIPA & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        AG & $\checkmark$ &  &  & $\checkmark$ \\ \cline{1-1}
        MiCE &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        GYC &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        POLYJUICE &  & $\checkmark$ &  &  \\ \cline{1-1}
        PIS &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        MEIO &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        R-LACE & $\checkmark$ &  & $\checkmark$ & $\checkmark$ \\ \cline{1-1}
        TEA & $\checkmark$ &  & $\checkmark$ & $\checkmark$ \\ \hline
    \end{tabular}}
    \caption{Caption.}
    \label{tab:C_mlm}
\end{table}

% address limitations of the survey at some point

% Unintended bias es más dificil de definir que bias en general -> requiere métodos más epecíficos para encontrarlo -> es más fácil pasar por alto otros sesgos

% Selección de papers es, en primer lugar, por su posible aplicación en búsqueda de sesgos, pero también por su relevancia "historica" en el campo de sesgos en NLP

% Dividir los métodos según la información que requieren para buscar los sesgos | exloratory - verification

