\section{Introduction}
\label{sec:intro}

% As the influence and scope of algorithms increase, academics, policymakers and journalists have raised concerns that these tools might inadvertedly encode and entech human biases \cite{measures-of-fairness} *

% predictive models in NLP are sensitive to (often unintended) bias \cite{predictive-bias-NLP} *

% bias can lead to social undesirable effects, by systematically mispredicting or underserving certain demographic goups \cite{predictive-bias-NLP}

% undetected and unassessed biases can lead to negative consequences \cite{predictive-bias-NLP} *

% Biased predictions emerge from hidden on neglected biases in the model or data \cite{bias-and-fairness-in-ML} *

% when applied in real-world applications, biased models have the potential for negative societal impacts \cite{sociodemographic-bias,NLP-risk-child-protective-service,critical-survey-on-bias} *

Alongside the increase in capacity and influence of algorithms, there is an increase in the concerns and risks of them inadvertedly perpetuating human biases~\cite{measures-of-fairness,Weapons-of-math-destruction}. This phenomenon is particularly evident in the context of neural networks developed for Natural Language Processing (NLP), as they learn directly from human-generated texts. The delegation of the decision-making process to these algorithms has the potential to engender negative societal impact if there are undetected or neglected biases~\cite{critical-survey-on-bias,NLP-risk-child-protective-service,sociodemographic-bias,bias-and-fairness-in-ML,predictive-bias-NLP}.

In this survey we present a brief review of bias and explainability methods, and discuss their applibility in prediction bias analysis. We will use the term predictive model (in NLP), to refer to any model that takes text as input, and produce a prediction, decision or classification as output (e.g., toxic text classification or spam filtering). An explainability method would be any method designed to provide insight about how the prediction is made, such as which variables are relevant or what information is being processed at a determined layer of the model.

Before deliniating the concept of prediction bias analysis, to ilustrate the risks of unassessed biases in predictive models in NLP, consider the following toy example:

\begin{displayquote}
A certain company is acused of favoring men over women in their hiring process. To solve this problem, the company decides to leave the process of a neural network, which determines if an applicant should be hired or not, based on their anonimized resume. The network is trained with the data of the previous processes, so it replicates the same biases. The next time the company is questioned for their hiring process, they respond that it is managed by an algorithm and, unlike humans, algorithms are objective, so if it more men than women are hired, it must be because there are more men better qualified for the job.
\end{displayquote}

% ----------------------------

% Mayor parte de los trabajos en sesgo se centran en medir o reducir un sesgo determinado. Ambos casos estan interconectados -> cuando se habla de des-sesgar, en realidad es reducir una métrica en particular

% El objetivo de esta survey no es decir que harmful o que no, sino sólo categorizar sesgos en general y cómo encontrarlos

In this example, the bias is the association of gender with qualification for the job, and it is originated by the use of biased training data. In Section \ref{sec:bias} we provide a more precise definition of bias in NLP and its origins.

% -------------------------------------------

\subsection{Social Bias and Predicion Bias}
\label{sec:intro:social_and_prediction_bias}

A substantial corpus of research has been dedicated to the examination of bias in predictibe models in NLP, and NLP algorithms in general. Most of them can be encapsuled in three categories: characterization of bias and its risks, measuring bias, and debiasing. The last two categories are closely intertwined, given that debiasing methods, which aim to remove a specific bias, often do so by reducing the metrics defined in the bias measuring literature. However, it has been observed that the metric reduction approach is more likely an elimination of symptoms rather than biases~\cite{fairness-survey,lipstick}.

In general, bias measuring works choose a bias, such as gender or racial bias, and propose a metric to quantify the presence of this bias in either the model representations or responses. These are referred in the literature as intrinsic and extrinsic metrics, respectively~\cite{quantifying-social-bias,sociodemographic-bias}. Intrinsic metrics have been found to not correlate with the responses of the model~\cite{intrinsic-not-correlate}, while extrinsic metrics are a subcategory fairness metrics~\cite{measures-of-fairness}, which, in turn, are metrics directly designed to measure the consequences and symptoms of biases (or other problematic elements), rather than the bias itself.

% ----------------------------

% we propose to address the problem from a new scope | give more weight to this other scope

% No son todos los trabajos en bias, pero se espera que sean los suficientemente relevantes y representativos

% No todos son directamente de bias, pero son metodologias que se pueden utilizar para el caso

% Decidir si un bias es harmful va despues de definir el bias -> se deben primero encontrar los sesgos, deseados o no

% Denuevo, el objetivo es discovery, no measuring or mitigation

% Aquí bias es bias y unintended bias es unintended bias

% Diferenciar bias de fairness

% Otra categoría útil puedeser sí es que el método sólo advierte de la presencia de sesgo o da algún indicio de su naturaleza -> métricas de fairness tenderían al primer caso

% poner running examples en la intro, usar retórica de fairness para justifica interés en sesgos

% bias discovery no es sólo señalar si existe sesgo o no, sino entregar algún indicio de caracterización del sesgo en cuestión

We argue that these problems may come from an indiscriminate use of the term bias to refer to two different terms: \textit{Social Bias} and \textit{Prediction Bias}. To better explain this idea, we will continue with the hiring model example:

%We propose an alternative approach to addressing the issue of bias in predictive models. Rather than measuring the extent of bias in a model, we endeavor to identify the biased associations that the model makes.

\begin{displayquote}
An organization against gender bias, dertermined to demonstrate that the model used by the company is biased, recolect of 100 applicants and their results. They find that men were accepted 4 times more than women, even when they have similar background. With this fairness test, the organization is sure that the model favors men over women, however, the company points out a key detail, the resumes are anonimized, so the model does not has any information regarding gender, and thus cannot be biased. The organization review the applications again, this time focusing on the diference between accepted and rejected resumes. They find that the rejected resumes tend to use longer sentences with more unique words, and that this characteristic is more frequent in women resumes\footnote{This toy example is based on the findings of Qu et al.\cite{gender-resume-differences}}. After editing the resumes to have a similar style and passing them to the model, it is found that the rejection rate is now equitative between men and women, probing that it was, in fact, biased.
\end{displayquote}

We have previously stated that, in the example, the bias is the association of gender with qualification for the job. To be precise, this is the social bias being replicated by the model. Here, even if a metric indicates that the answers of the model are biased, it does not explain why it happens, because it is only measuring the social bias in its responses. After examining a set of input-output samples, it is found a correlation between a single attribute of the input and a particular output, an association can be called a prediction bias. We define social and prediction biases as follows:

\begin{itemize}
    \item \textbf{Social Bias:} Prejudices and associations, related to social groups, made by humans. It can be learned ans expressed by predictive model. In the example this would be gender discrimination.
    \item \textbf{Prediction Bias:} Association between variables encoded in the computation of the model. In the example this would be the correlation between the variables sentence lenght and word diversity and the outcome.
\end{itemize}

Social bias can ogirinate and be a consequence of prediction bias, but prediction bias does not implicate social bias. We provide further discussion and previously definitions of the concept of prediction bias in Section \ref{sec:bias}.

\subsection{Prediction Bias Analysis}

We denominate this procedure of finding associations as bias detection. In the example, the association is identified through manual examination. However, it would be preferable to have a mechanism capable of automatically detecting bias. 

In this short survey we review some methods that can be eployed, or repurposed, to perform this task. The scope of the survey is restricted to methods that can be applied on predictive models with transformer architecture. We divide the methods in two categories: examination methods (Section \ref{sec:examination}), that examinates the representations used or generated by the model and its operations, and example generation methods (Section \ref{sec:examples}), that generates examples that might show the biases of the model. We also indicate if the methods are useful for a confirmatory or exploratory bias analysis. The definitions of both analyses are provided in Section \ref{sec:indirect-effect}.

\begin{table}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|l|cc|cc|}
        \hline
         & \multicolumn{1}{c|}{Examination} & \multicolumn{1}{c|}{Example Generation} & \multicolumn{1}{c|}{Confirmatory} & \multicolumn{1}{c|}{Exploratory} \\ \hline
        WEAT & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        SEAT & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        CEAT & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        RIPA & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        AG & $\checkmark$ &  &  & $\checkmark$ \\ \cline{1-1}
        MiCE &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        GYC &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        POLYJUICE &  & $\checkmark$ &  &  \\ \cline{1-1}
        PIS &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        MEIO &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        R-LACE & $\checkmark$ &  & $\checkmark$ & $\checkmark$ \\ \cline{1-1}
        TEA & $\checkmark$ &  & $\checkmark$ & $\checkmark$ \\ \hline
    \end{tabular}}
    \caption{Caption.}
    \label{tab:C_mlm}
\end{table}

% address limitations of the survey at some point

% Unintended bias es más dificil de definir que bias en general -> requiere métodos más epecíficos para encontrarlo -> es más fácil pasar por alto otros sesgos

% Selección de papers es, en primer lugar, por su posible aplicación en búsqueda de sesgos, pero también por su relevancia "historica" en el campo de sesgos en NLP

% Dividir los métodos según la información que requieren para buscar los sesgos | exloratory - verification

