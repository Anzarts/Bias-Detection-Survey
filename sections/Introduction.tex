\section{Introduction}
\label{sec:intro}

% As the influence and scope of algorithms increase, academics, policymakers and journalists have raised concerns that these tools might inadvertedly encode and entech human biases \cite{measures-of-fairness} *

% predictive models in NLP are sensitive to (often unintended) bias \cite{predictive-bias-NLP} *

% bias can lead to social undesirable effects, by systematically mispredicting or underserving certain demographic goups \cite{predictive-bias-NLP}

% undetected and unassessed biases can lead to negative consequences \cite{predictive-bias-NLP} *

% Biased predictions emerge from hidden on neglected biases in the model or data \cite{bias-and-fairness-in-ML} *

% when applied in real-world applications, biased models have the potential for negative societal impacts \cite{sociodemographic-bias,NLP-risk-child-protective-service,critical-survey-on-bias} *

Alongside the increase in capacity and influence of algorithms, there is an increase in the concerns and risks of them inadvertedly perpetuating human biases~\cite{measures-of-fairness,Weapons-of-math-destruction}. This phenomenon is particularly evident in the context of neural networks developed for Natural Language Processing (NLP), as they learn directly from human-generated texts. The delegation of the decision-making process to these algorithms has the potential to engender negative societal impact if there are undetected or neglected biases~\cite{critical-survey-on-bias,NLP-risk-child-protective-service,sociodemographic-bias,bias-and-fairness-in-ML,predictive-bias-NLP}. We will use the term predictive model (in NLP), to refer to any model that takes text as input, and produce a prediction, decision or classification as output (e.g. toxic text classification or spam filtering). To ilustrate the risks of unassessed biases in predictive models in NLP, consider the following toy example.

\begin{displayquote}
A certain company is acused of favoring men over women in their hiring process. To solve this problem, the company decides to leave the process of a neural network, which determines if an applicant should be hired or not, based on their anonimized resume. The network is trained with the data of the previous processes, so it replicates the same biases. The next time the company is questioned for their hiring process, they respond that it is managed by an algorithm and, unlike humans, algorithms are objective, so if it more men than women are hired, it must be because there are more men better qualified for the job.
\end{displayquote}

% ----------------------------

% Mayor parte de los trabajos en sesgo se centran en medir o reducir un sesgo determinado. Ambos casos estan interconectados -> cuando se habla de des-sesgar, en realidad es reducir una métrica en particular

% El objetivo de esta survey no es decir que harmful o que no, sino sólo categorizar sesgos en general y cómo encontrarlos

In the example, the bias is the association of gender with qualification for the job, and it is originated by the use of biased training data. In Section \ref{sec:bias} we provide a more precise definition of bias in NLP and its origins.

A substantial corpus of research has been dedicated to the examination of prediction bias in NLP, and biases in NLP in general. Most of them can be encapsuled in three categories: characterization of bias and its risks, measuring bias, and debiasing. The last two categories are closely intertwined, given that debiasing methods, which aim to remove a specific bias, often do so by reducing the metrics defined in the bias measuring literature. However, it has been observed that the metric reduction approach is more likely an elimination of symptoms rather than biases~\cite{fairness-survey,intrinsic-not-correlate,lipstick}.

In general, bias measuring works choose a social bias, such as gender or racial bias, and propose a metric to quantify the presence of this bias in either the model representations or responses. These are referred in the literature as intrinsic and extrinsic metrics, respectively~\cite{quantifying-social-bias,sociodemographic-bias}. This concept is closely related to that of fairness metrics~\cite{measures-of-fairness}, which, in turn, are metric directly designed to measure the consequences and symptoms of biases (or other problematic elements), rather than the bias itself.

% ----------------------------

% we propose to address the problem from a new scope | give more weight to this other scope

% No son todos los trabajos en bias, pero se espera que sean los suficientemente relevantes y representativos

% No todos son directamente de bias, pero son metodologias que se pueden utilizar para el caso

% Decidir si un bias es harmful va despues de definir el bias -> se deben primero encontrar los sesgos, deseados o no

% Denuevo, el objetivo es discovery, no measuring or mitigation

% Aquí bias es bias y unintended bias es unintended bias

% Diferenciar bias de fairness

% Otra categoría útil puedeser sí es que el método sólo advierte de la presencia de sesgo o da algún indicio de su naturaleza -> métricas de fairness tenderían al primer caso

% poner running examples en la intro, usar retórica de fairness para justifica interés en sesgos

% bias discovery no es sólo señalar si existe sesgo o no, sino entregar algún indicio de caracterización del sesgo en cuestión

We propose an alternative approach to addressing the issue of bias in predictive models. Rather than measuring the extent of bias in a model, we endeavor to identify the biased associations that the model makes. To better explain this idea, we will continue with the hiring model example.

\begin{displayquote}
An association against gender bias, dertermined to demonstrate that the model used by the company is biased, recolect of 100 applicants and their results. They find that men were accepted 4 times more than women, even when they have similar background. With this fairness test, the association is sure that the model favors men over women, however, the company points out a key detail, the resumes are anonimized, so the model does not has any information regarding gender, and thus cannot be biased. The association review the applications again, this time focusing on the diference between accepted and rejected resumes. They find that the rejected resumes tend to use longer sentences with more unique words, and that this characteristic is more frequent in women resumes\footnote{This toy example is based on the findings of Qu et al.\cite{gender-resume-differences}}. After editing the resumes to have a similar style and passing them to the model, it is found that the rejection rate is now equitative between men and women, probing that it was, in fact, biased.
\end{displayquote}

Here, even if a metric indicates that the answers of the model are biased, it does not explain why it happens. After examining a set of input-output samples, it is found a correlation between a single attribute of the input and a particular output, an association can be called a bias. We denominate this procedure of finding associations as bias detection. In the example, the association is identified through manual examination. However, it would be preferable to have a mechanism capable of automatically detecting bias. 

In this short survey we review some methods that can be eployed, or repurposed, to perform this task. The scope of the survey is restricted to methods that can be applied on predictive models with transformer architecture. We divide the methods in two categories: examination methods (Section \ref{sec:examination}), that examinates the representations used or generated by the model and its operations, and example generation methods (Section \ref{sec:examples}), that generates examples that might show the biases of the model. We also indicate if the methods are useful for a confirmatory or exploratory bias analysis. The definitions of both analyses are provided in Section \ref{sec:indirect-effect}.

\begin{table}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|l|cc|cc|}
        \hline
         & \multicolumn{1}{c|}{Examination} & \multicolumn{1}{c|}{Example Generation} & \multicolumn{1}{c|}{Confirmatory} & \multicolumn{1}{c|}{Exploratory} \\ \hline
        WEAT & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        SEAT & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        CEAT & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        RIPA & $\checkmark$ &  & $\checkmark$ &  \\ \cline{1-1}
        AG & $\checkmark$ &  &  & $\checkmark$ \\ \cline{1-1}
        MiCE &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        GYC &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        POLYJUICE &  & $\checkmark$ &  &  \\ \cline{1-1}
        PIS &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        MEIO &  & $\checkmark$ &  & $\checkmark$ \\ \cline{1-1}
        R-LACE & $\checkmark$ &  & $\checkmark$ & $\checkmark$ \\ \cline{1-1}
        TEA & $\checkmark$ &  & $\checkmark$ & $\checkmark$ \\ \hline
    \end{tabular}}
    \caption{Caption.}
    \label{tab:C_mlm}
\end{table}

% address limitations of the survey at some point

% Unintended bias es más dificil de definir que bias en general -> requiere métodos más epecíficos para encontrarlo -> es más fácil pasar por alto otros sesgos

% Selección de papers es, en primer lugar, por su posible aplicación en búsqueda de sesgos, pero también por su relevancia "historica" en el campo de sesgos en NLP

% Dividir los métodos según la información que requieren para buscar los sesgos | exloratory - verification

