\section{Examination Methods}
\label{sec:examination}

In this section, we review some methods that can be employed to gain insight on Prediction Bias through the examination of the components of the prediction process, such as the embeddigs or neuron activations. In Section \ref{sec:examination:embeddings} we discuss methods of Embedding Association, that compare the vector representations employed by the model; in Section \ref{sec:examination:erasure} we discuss methods for Concept Erasure, that seek to erase specific information from the representations while leaving the rest of it intact; and in Section \ref{sec:examination:ablation} we discuss methods for Model Ablation, that seek to remove parts of the model, such as neurons or conections, without harmimng a given behavior.

\subsection{Embedding Association}
\label{sec:examination:embeddings}

This category encompas methods that can be employed to detect biases in the word representations used by the model. These can be either the word-embeddings used in the input, or the contextualized embeddings generated by the model. As its name indicates, methods of this family seek to uncover associations between embbedings, with the subjacent objective of identifying information encoded in them.

These are the weakest methods for Prediction Bias Analysis within the scope of this survey. That is because most of them can only confirm that some specific information is present in the embedding, but cannot this information is employed for the model's prediction, or if it employed at all. Despite of this, we decided to include these methods, because they represent a prominent portion of the bias evaluation literature, and because, in theory, the embeddings are ajusted to encode the information that the model needs for its prediction. This means that, if some information is encoded in the embeddigs, then there is a high chance that it is employed for the prediction in some way.

\subsubsection{Word-Embedding Association Test}

% RIPA \cite{RIPA} hace lo mismo que WEAT, pero con inner product en vez de similitud coseno
% como WEAT s칩lo estudia relaciones entre los embeddings y no con la predicci칩n de un modelo, no hay an치lisis causal
% en alguna survey hay una cita de que WEAT no correlaciona con los sesgos del modelo
% es sensible a malas definiciones de los sets -> fallar o llebar a malas conclusiones

The Word-Embedding Association Test (WEAT)~\cite{WEAT} is one of the most influential work addressing bias encoded in word-embeddings. WEAT is an adaptation of the Intrinsic-Association Test used in social psychology, to measure stereoty-related bias in word-embeddings. Given 2 sets $X, Y$ of target words (e.g. professions) and 2 sets $A, B$ of attribues words (e.g. gender nouns), WEAT provides a metric (equations \ref{eq:WEAT1} and \ref{eq:WEAT2}) that measures the differential association of the two sets of target words $X, Y$ with the attributes $A, B$, where the association between words is defined as the cosine similarity.

\begin{equation}
\label{eq:WEAT1}
    \text{WEAT}(A,B,X,Y) = \frac{\mean_{x\in X}s(x,A,B)-\mean_{y\in Y}s(y,A,B)}{\sd_{w\in X\cup Y}s(w,A,B)}
\end{equation}

\begin{equation}
\label{eq:WEAT2}
    s(w,A,B) = \mean_{a\in A}\text{cossim}(w,a) - \mean_{b\in B}\text{cossim}(w,B)
\end{equation}

WEAT was designed to be applied in contexts where the words of $X$ and $Y$ should be equaly associated to the words of both $A$ and $B$. If, for example, it is found $A$ is more associated with $X$ than $Y$, it said that the embeddings are biased.

Many works have adapted WEAT to work in other escenarios. For instance, SEAT~\cite{SEAT} measures association in sentences, replacing the word-embeddings by sentence-embeddings, and CEAT~\cite{CEAT} measures association in contextualized embedding, by computing WEAT $N$ times with random contextualized embeddings, from different sentences containing words from the target and attribute sets, and then analizing the resulting distribution. There are also alternative formulations of WEAT for the same context, such as RIPA~\cite{RIPA}, that propose to use the inner product instead of cosine similarity to measure association.

The evaluation performed by WEAT-based methods is closer to a measurement of the extent to which Social Bias is replicated in the embbedings, than to an assessment of Prediction Bias. This is mainly because they do not find any relation between the input and the output of the model. For Prediction Bias Analysis, these methods can be repurposed as a criterion to generate the sets $X,Y$, instead of evaluating them. For example, solving the double maximization:

\begin{equation}
    \label{eq:WEAT3}
    \max_{X\in V^*\setminus Y}\max_{Y\in V^*\setminus X}\text{WEAT}(A,B,X,Y)
\end{equation}

where $V^*$ is a target vocabulary. Then these sets can be employed to measure the difference in the model's responses for each of them, which can provide some information regarding the existence of a Prediction Bias related to the association between the sets. This is still a basic analisys, that could be done withput adding WEAT to the equation. Given that WEAT-based methods can only look for pre-determined associations, they fall under the Confirmatory Method category.

% CEAT Also developed de Intersectional Bias Detection (IBD) method, to automatically identify biases without rellying on pre-defined sets
% Intersectional refers to individuals that are in the intersection of two groups
% just run wefat over different attributes and words, and select the words with values over a threshold

% --------------------------------

\subsubsection{Analogy Generation}

% man is to computer etc \cite{man-is-to-computer}
% analogy generator: given words a, b, search a pair x, y that fit in the analogy a is to x as b is to y
% filter by the metric: cossim(a-b,x-y) if ||x-y|| < d else 0

% find gender sub-space by applying PCA to she - he like vectors -> gender pair difference
% then test if words are similars to this vector

One interesting feature of word-embeddings is that they have been found able to express words relation through vector difference~\cite{embedding-relations-1,embedding-relations-2}. For example, the different between the embeddings for man and woman is similar to the difference between the embeddings for king and queen. This can be expressed in an analogy of the form ``man is woman as king is to queen''. Given a pair of words $x$ and $y$, the method of analogy generation~\cite{man-is-to-computer} consist in looking for pairs $(a,b)$ that might fit in the analogy ``$x$ is to $y$ as $a$ is to $b$''. To do this, each pair $(a,b)$ is assigned a score defined by:

\begin{equation}
    S_{x,y}(a,b) = \begin{cases} \text{cossim}(x-y,a-b) & \text{if   } \|a-b\| \leq \delta \\ 0 & \text{if   } \|a-b\| > \delta \end{cases}
\end{equation}

where $\delta$ is a threshold for the distance between $a$ and $y$.

The uses and limitation of Analogy Generation for Prediction Bias Analysis are basically the same as WEAT, as it designed for establishing associationn between inputs.

\subsection{Concept Erasure}
\label{sec:examination:erasure}

Given a set of vector representations $X=\{x_i\}^N_{i=1}\subseteq \mathbb{R}^d$ (e.g., word-embeddings) and a set of response variables $C=\{y_i\}^N_{i=1}$ that indicates a concept in the vectors (e.g., gender, verb tenses), Concept Erasure methods implement some function $r: \mathbb{R}^d\to \mathbb{R}^{d'}$, such that the resulting vectors $r(x_i)$ preserve as much information as possible, while not being predictive of concept $C$.

Concept Erasure offers the input-output relationship assessment that Embedding Association lacks. While Embedding Association can prove the presence of a concept $C$ in the representations, Concept Erasure, by removing $C$, provides a way to evaluate is the concept is relevant for the model's predictions.

% - - - - - - - - - - - - - - - - - - - - - - - -

\subsubsection{R-LACE}

% puede encontrar sesgos al comparar instancias similares en la proyecci칩n y distintas en el original

Relaxed Linear Adversal Concept Erasure (R-LACE)~\cite{R-LACE} is a method designed for the task of Concept Erasure in the word-embbedings or inner representations. To erase a concept, R-LACE first finds a subspace $B\subseteq \mathbb{R}^d$ that contains the information of the target concept, within the representations, and then projects the vector representations into the ortogonal complement of $B$. The subspace $B$ is determined by solving the minmax problem:

\begin{equation}
    \min_{\theta}\max_{P}\sum^N_{i=1}\mathcal{L}(y_i,g^{-1}(\theta^T P x_i))
\end{equation}

where $f_\theta(x) = g^{-1}(\theta^T x)$ is a generalized linear model, with parameters $\theta$ and link function $g$, $\mathcal{L}$ is a loss function, and $P$ is a $d\times d$ ortogonal projection matrix that neutralizes a rank $k$ subspace, with $k$ being an hyper-parameter of the algorithm. R-LACE can be expanded to work with non-linear subspaces, by applying a kernel on $f_\theta$~\cite{kernelized-concept-erasure}.

Note that the definition of R-LACE does not require to have an explicit definition of the concept to be erased, just to know the response variables. R-LACE can be repurposed for bias detection, at the level of the inner representations, for both Confirmatory Analysis and Exploratory Analysis.

Let $X$ be the inner representations given by some layer of a predictive model $M$, $C$ a response variable associated to a given concept, and $r(X) = \{r(x_i)\}^N_{i=1}$ the resulting vectors after applying R-LACE on $X$ to erase $C$. R-LACE can be employed for Confirmatory Analysis if $C$ indicates a known features of the input. In this case, if the responses of $M$ over $X$ cannot be replicated for $r(X)$, that would indicate that $C$ has a direct effect on the model's predictions.

To employ R-LACE for Confirmatory Analysis, the response variable $C$ can be defined an indicator of whether a representation $x_i$ is assigned to a given prediction $y$ or not. If there are two instances $a$ and $b$, such that their representations are similar after applying R-LACE, $r(x_a)\approx r(x_b)$, but not before, $x_a \not\approx x_b$, then a Prediction Bias, related to the output $y$, might be found difference between $a$ and $b$.

\subsection{Model Ablation}
\label{sec:examination:ablation}

The objective of Model Ablation methods is to modify the behavior of a model by removing or nullifying a subset of its components, such as neuron or connections between them. Model Ablation can by employed for Prdiction Bias Analysis following a similar logic to application of Concept Erasure. If a determined behavior of the model, with respect to a set of inputs $\mathcal{D}$, is nullified after ablating a component $G_\mathcal{D}$, it can be inferred that $G_\mathcal{D}$ computes a Prediction Bias associated to $\mathcal{D}$.

\subsubsection{Targeted Edge Ablation}

Targeted Edge Ablation (TEA)~\cite{circuit-breaking} is a technique designed to remove an especific behavior of the model, by ablating a small number of edges, or pathways, between its components. In this context, given a model $M$ and a loss function $\mathcal{L}$ (that is not necessarily the one used to train $M$), a behavior is defined as a set of inputs $\mathcal{D}$ on which $M$ achieves low loss. The task of behavior removal is defined as modifying $M$ to create another model $M'$ that achieves high loss on $\mathcal{D}$, without a significant increase of loss on inputs outside $\mathcal{D}$.

To ablate $M$, first is necessary to choose at what level of granularity represent the model's computations, and write the graph $G$ that describes $M$ at that specific level (e.g. represent $M$ as a graph of attention heads and feed forward layers). Then the ablated edges in $G$ are determined by solving:

\begin{equation}
    \min_{W} \mathcal{L}(G_W, D_{\text{train}}) -  \alpha\mathcal{L}(G_W, \mathcal{D}) + \lambda(t)R(W)
\end{equation}

where $G_W$ is $M$ with a mask $W$ applied over the edges of $G$, $D_{\text{train}}$ is a set of train data, disjoint to $\mathcal{D}$, $R$ is a regularization function, $\alpha$ is a constant, and $\lambda(t)$ a regularization weight that increase over time. The mask $W$ assigns to each edge $e=(A,B)$ of $G$ a weight $w_e\in[0,1]$, such that node $B$ recieves the following combination of the original value $v_A$ and the ablated value $\mu_A$ from node $A$:

\begin{equation}
    w_ev_A + (1-w_e)\mu_A
\end{equation}

After the optimization is finished, the edges whose weight does not surpase a specified threshold are ablated.

If $\mathcal{D}$ is set to represent a biased behavior (Social Bias), and TEA is capable of effectively removing it from the model, then that would comfirm that $M$ computes the specified bias. Moreover it would indentify the components of $M$ that compute the Prediction Bias. If it is complementes with a method to analize the inner representations, it could help to identify the input's features associated to the bias, by comparing the inner representations before and after the ablation.%\cite{geometry-of-truth}

% =================================================

\section{Example Generation}
\label{sec:examples}

In this section, we review some methods that can be employed to gain insight on Prediction Bias, through the the generation of examples that produce a determined response on the model. In Section \ref{sec:examples:activation_max} we discuss methods for Activation Maximization, that seek to generate inputs that maximize some output logit or neuron activation of the model; and in Section \ref{sec:examples:counterfactual} we discuss methods for Counterfactual Examples Generation, that generate samples that are similar to a given input while attaining a different outcome.

\subsection{Activation Maximization}
\label{sec:examples:activation_max}

Activation Maximization methods aim to generate input samples that maximize the activation of some neuron of the model, in order to uncover to which features this neuron is sensible. Given a neuron whose activation is interpretable, such as the prediction layer's logits, Activation Maximization can be employed for Exploratory Analysis, to seek for features of the inputs that are associated with the activation of the target neuron.

Here we review two methods for Activation Maximization: Preferred Input Synthesis~\cite{synthetisizing-Nguyen} and Evolutionary Input Optimization~\cite{synthetisizing-Barbalau}. Both of them generate its examples with a generative model, a neural network $G: Z\to X$ that maps points from a vector space, called latent space, into text. The former following a white-box approach and the later a black-box approach. Despite this difference, their appliability of Prediction Bias Analysis is the same.

\subsubsection{Preferred Input Synthesis}

The Preferred Input Synthesis (PIS)~\cite{synthetisizing-Nguyen} method generates inputs that maximize the activation of a target neuron (including the output logits), via optimization at the level of the latent space of a generative model. Given a predictive model $M:X\to Y$ and a generative model $G:Z\to X$, denoting by $M_h$ the activation of the target neuron $h$, PIS generate samples by solving:

\begin{equation}
    \argmax_{z\in Z}M_h(G(z)) - \lambda\|z\|
\end{equation}

where $\lambda$ is a regularization term and the optimization is performed with gradient descend. PIS was originally formulated for image generation, but can easily be re-adapted for text.


\subsubsection{Momentum Evolutionary Input Optimization}

The Evolutionary Input Optimization (MEIO)~\cite{synthetisizing-Barbalau} method is a generic framework for Activation Maximization that propose a model-agnostic approch, with a zero-order optimization on the latent space of a generative model. Given a predictive model $M:X\to Y$, a generative model $G:Z\to X$, and a target prediction $y\in Y$, MEIO generate samples by solving:

\begin{equation}
    \min_{z\in Z}\mathcal{L}(M(G(z)), y)
\end{equation}

where $\mathcal{L}$ is a loss function. The optimization is performed via an evolutionary strategy with momentum updates, that involves iteratively updating a set of candidate solutions, by adding to them an idenpdently sampled gaussian noise, which is combined with noised added in the previous itariton (the momentum).

\subsection{Counterfactual Examples}
\label{sec:examples:counterfactual}

In machine learning, a counterfactual explanation is defined as an example that illustrates how a different input would result in a different output~\cite{NLP-posthoc-interpret-survey}. In the context of predictive models for NLP, if an input text $X$ gets an output $y$ by the model, a counterfactual example would be any text $X'$, similar to $X$, that yields a different output $y'$~\cite{counterfactual-explanations,NLP-counterfactual-survey}. These modifications on the input, that are both minimal and enought to change the prediction, can highlight which features of the input are participating in a Prediction Bias. In this section we review three differnt methods for counterfactual example generation: POLYJUICE~\cite{polyjuice}, MiCE~\cite{MiCE}, and GYC~\cite{GYC}.

\subsubsection{POLYJUICE}

The POLYJUICE~\cite{polyjuice} method employs a fine-tuned GPT-2~\cite{GPT-2} model to generate counterfactual examples. The model recieves an input text $X$ and a perturbation instruction, such as negation, insertion or deletion, and returns a modified version of $X$, following the given instruction. Setting the perturbation instruction to modify a specific feature, POLYJUICE can be employed to perform a weak Confirmatory Analysis.

\subsubsection{MiCE}

The Minimal Contrastive Editing (MiCE)~\cite{MiCE} method generate counterfactual examples via a masked language model, denomined editor model, that is trained to fill the masked spaces in an intput text with tokens, such that the resulting text would obtain a given prediction by a predictive model $M$. The editor model recieves the masked text and the target label as inputs. During training, the top $n_1\%$ tokens with highest gradient attribution~\cite{gradient-attribution}, towards the target prediction, are masked. To generate the counterfactuals, the percentage of masked tokens is varied between $0\%$ and $55\%$, using binary search to find the optimal percentage and beam search to keep track of the edits. The generation process stops once an edit changes the prediction to the target.

In contrast to POLYJUICE, MiCE can be employed mainly for Exploratory Analysis. In particular, given an output variable $y$ and an input text $x$, such that $M(x)\neq y$, can generate a set token flips that result in the model shifting its prediction to $y$. These flips might highlight features of the input that $M$ associate with $y$. However, identifying those features would require an exhaustive qualitative analysis or a very well defined and limited task-dataset framework.

\subsubsection{GYC}

The GYC~\cite{GYC} method generates $k$ counterfactual examples, for a given input text $X$ and a condition $C$, through controlled text generation. It does not require to train or fine-tune a model. The method consists in modelating the distribution $p(\tilde{y}|X, C)$, for next token prediction, where the condition $C$ can be any restriction over the text, such as a class label.

Let $LM$ be a language model transformer; $LM$ generates a token $y_t$ conditioned on the past tokens $y_{<t}=\{y_i\}^{t-1}_{i=0}$ as follows:

\begin{equation}
    \begin{split}
        o_{t}, H_{t} &= \text{LM}(y_{t-1}, H_{t-1})\\
        y_t &\sim \text{Categorical}(o_t)
    \end{split}
\end{equation}

where $H_{t-1}$ is the history matrix that captures the dependency of $y_{t-1}$ on past tokens and $o_t$ are the output logits, that generate the categorical distribution from which $y_t$ is sampled. To generate a text conditioned on $C$, $H$ is perturbed two times. The first perturbation is to create $\tilde{H}_t$, which enforces the reconstruction of $X$, and next is to create $\hat{H}_t$, which enforces the condition $C$. The perturbations are determined by minimizing a linear combination of three loss functions is employed, one for reconstruction and one enforce the condition, plus another one to ensure diversity. The reconstruction loss maximizes the log probability of the input text, the condition loss maximizes a score assosiated to the condition and the diversity loss maximizes the entropy of the generated logits.

GYC can be employed for both Confirmatory Analysis and Exploratory Analysis, depending on how the condition $C$ is defined. If $C$ enforces the aparition of a certain feature, it can be used to measure the effect of this feature on the output. If $C$ enforces a behavior of the model, it can be used to find features that trigger that behavior. For the Exploratory Analysis, it has the advantage over methods like MiCE that $C$ can enforce more complex conditions that just attaining a given output, such as not inserting already known biases\footnote{e.g., not inserting insults if we are looking for unintended bias in toxic comment classification.}. This reduces the extent to which qualitative analysis is needed to identify the biases.

