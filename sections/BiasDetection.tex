\section{Examination Methods}
\label{sec:examination}

% Mechanism porque examina los mecanismos con que opera el modelo, o cómo son codificadas las palabras -> no confundir con mechanistic interpretability

\subsection{Embedding Association}
\label{sec:examination:embeddings}

This category encompas methods that can be employed to detect biases in the word representations used by the model. These can be either the word-embeddings used in the input, or the contextualized embeddings generated by the model.

\subsubsection{Word-Embedding Association Test}

% RIPA \cite{RIPA} hace lo mismo que WEAT, pero con inner product en vez de similitud coseno

% como WEAT sólo estudia relaciones entre los embeddings y no con la predicción de un modelo, no hay análisis causal
% en alguna survey hay una cita de que WEAT no correlaciona con los sesgos del modelo
% es sensible a malas definiciones de los sets -> fallar o llebar a malas conclusiones

The Word-Embedding Association Test (WEAT)~\cite{WEAT} is one of the most influential work addressing bias encoded in word-embeddings. WEAT is an adaptation of the Intrinsic-Association Test used in social psychology, to measure stereoty-related bias in word-embeddings. Given 2 sets $X, Y$ of target words (e.g. professions) and 2 sets $A, B$ of attribues words (e.g. gender nouns), WEAT provides a metric (equations \ref{eq:WEAT1} and \ref{eq:WEAT2}) that measures the differential association of the two sets of target words $X, Y$ with the attributes $A, B$, where the association between words is defined as the cosine similarity.

\begin{equation}
\label{eq:WEAT1}
    \text{WEAT}(A,B,X,Y) = \frac{\mean_{x\in X}s(x,A,B)-\mean_{y\in Y}s(y,A,B)}{\sd_{w\in X\cup Y}s(w,A,B)}
\end{equation}

\begin{equation}
\label{eq:WEAT2}
    s(w,A,B) = \mean_{a\in A}\text{cossim}(w,a) - \mean_{b\in B}\text{cossim}(w,B)
\end{equation}

WEAT was designed to be applied in contexts where the words of $X$ and $Y$ should be equaly associated to the words of both $A$ and $B$. If, for example, it is found $A$ is more associated with $X$ than $Y$, it said that the embeddings are biased.

Many works have adapted WEAT to work in other escenarios. For instance, SEAT~\cite{SEAT} measures association in sentences, replacing the word-embeddings by sentence-embeddings, and CEAT~\cite{CEAT} measures association in contextualized embedding, by computing WEAT $N$ times with random contextualized embeddings, from different sentences containing words from the target and attribute sets, and then analizing the resulting distribution. There are also alternative formulations of WEAT for the same context, such as RIPA~\cite{RIPA}, that propose to use the inner product instead of cosine similarity to measure association.

WEAT-based methods can be employed for detecting bias, by defining a threshold for the bias metric delimit from which point the embeddings are considered to be biased. However these methods are constricted by the requierement of defining the target and attribute sets. WEAT can only look for pre-determined associations, and is subseptible to error if word sets are not well defined.

A possible solution to the limitation of the target or attribute sets, could be to develope an algorithm that authomatically generates these sets, similiar to how the Intersectional Bias Detection method iterate~\cite{CEAT}, proposed by the authors of CEAT, iterate over different combinations of subsets of the attributes to find biases associated to individuals that are in the intersection of two groups. 

% CEAT Also developed de Intersectional Bias Detection (IBD) method, to automatically identify biases without rellying on pre-defined sets
% Intersectional refers to individuals that are in the intersection of two groups
% just run wefat over different attributes and words, and select the words with values over a threshold

% --------------------------------

\subsubsection{Analogy Generation}

% man is to computer etc \cite{man-is-to-computer}
% analogy generator: given words a, b, search a pair x, y that fit in the analogy a is to x as b is to y
% filter by the metric: cossim(a-b,x-y) if ||x-y|| < d else 0

% find gender sub-space by applying PCA to she - he like vectors -> gender pair difference
% then test if words are similars to this vector

One interesting feature of word-embeddings is that they have been found able to express words relation through vector difference~\cite{embedding-relations-1,embedding-relations-2}. For example, the different between the embeddings for man and woman is similar to the difference between the embeddings for king and queen. This can be expressed in an analogy of the form ``man is woman as king is to queen''. Given a pair of words $x$ and $y$, the method of analogy generation~\cite{man-is-to-computer} consist in looking for pairs $(a,b)$ that might fit in the analogy ``$x$ is to $y$ as $a$ is to $b$''. To do this, each pair $(a,b)$ is assigned a score defined by:

\begin{equation}
    S_{x,y}(a,b) = \begin{cases} \text{cossim}(x-y,a-b) & \text{if   } \|a-b\| \leq \delta \\ 0 & \text{if   } \|a-b\| > \delta \end{cases}
\end{equation}

where $\delta$ is a threshold for the distance between $a$ and $y$. Analogy generation can be employed for bias detection at the level of the word-embeddings, but have similar issues to the ones of WEAT, as it requires to pre-define a set of candidate words for $a$ and $b$, which can be under-representative.

\subsection{Concept Erasure}
\label{sec:examination:erasure}

\subsubsection{R-LACE}

% puede encontrar sesgos al comparar instancias similares en la proyección y distintas en el original

Relaxed Linear Adversal Concept Erasure (R-LACE)~\cite{R-LACE} is method designed for the task of concept erasure in the representations. That is, given a set of vector representations $X=\{x_i\}^N_{i=1}\subseteq \mathbb{R}^d$ (e.g. word-embeddings) and a set of response variables $Y=\{y_i\}^N_{i=1}$ that indicates a concept in the vectors (e.g. gender), implement some function $r: \mathbb{R}^d\to \mathbb{R}^{d'}$, such that the resulting vectors $r(x_i)$ preserve as much information as possible, while not being predictive of concept $Y$.

To erase a concept, R-LACE finds a subspace $B\subseteq \mathbb{R}^d$ that contains the information of the target concept, within the representations, and project the vector representations to the ortogonal complement of $B$. The subspace $B$ is determined by solving the minmax problem:

\begin{equation}
    \min_{\theta}\max_{P}\sum^N_{i=1}\mathcal{L}(y_i,g^{-1}(\theta^T P x_i))
\end{equation}

where $f_\theta(x) = g^{-1}(\theta^T x)$ is a generalized linear model, with parameters $\theta$ and link function $g$, $\mathcal{L}$ is a loss function, and $P$ is a $d\times d$ ortogonal projection matrix that neutralizes a rank $k$ subspace, with $k$ being an hyper-parameter of the algorithm.

Note that the definition of R-LACE does not require to have an explicit definition of the concept to be erased, just to know the response variables. R-LACE can be repurposed for bias detection, at the level of the inner representations, by erasing the concept that determines a particular prediction. R-LACE can be expanded for non-linear subspaces by applying a kernel on $f_\theta$~\cite{kernelized-concept-erasure}.

Let $X$ be the inner representations given by some inner layer of a predictive model, $Y$ a response variable that indicates if a representation $x_i$ is assigned to particular prediction or not by the model, and $r(X) = \{r(x_i)\}^N_{i=1}$ the resulting vectors after applying R-LACE on $X$ to erase $Y$. If there are two instances $a$ and $b$, such that their representations are similar after applying R-LACE, $r(x_a)\approx r(x_b)$, but not before, $x_a \not\approx x_b$, then the difference between $a$ and $b$ might show the prediction bias.

\subsection{Model Ablation}
\label{sec:examination:ablation}

\subsubsection{Targeted Edge Ablation}

Targeted Edge Ablation (TEA)~\cite{circuit-breaking} is a technique designed to remove an especific behavior of the model, by ablating a small number of edges, or pathways, between its components. In this context, given a model $M$ and a loss function $\mathcal{L}$ (that is not necessarily the one used to train $M$), a behavior is specified as a set of inputs $\mathcal{D}$ on which $M$ achieves low loss. The task of behavior removal is defined as modifying $M$ to create another model $M'$ that achieves high loss on $\mathcal{D}$, without a significant increase of loss on inputs outside $\mathcal{D}$.

To ablate $M$, first is necessary to choose at what level of granularity represent the model's computations, and write the graph $G$ that describes $M$ at that specific level (e.g. represent $M$ as a graph of attention heads and feed forward layers). Then the ablated edges in $G$ are determined by solving:

\begin{equation}
    \min_{W} \mathcal{L}(G_W, D_{\text{train}}) -  \alpha\mathcal{L}(G_W, \mathcal{D}) + \lambda(t)R(W)
\end{equation}

where $G_W$ is $M$ with a mask $W$ applied over the edges of $G$, $D_{\text{train}}$ is a set of train data, disjoint to $\mathcal{D}$, $R$ is a regularization function, $\alpha$ is a constant, and $\lambda(t)$ a regularization weight that increase over time. The mask $W$ assigns to each edge $e=(A,B)$ of $G$ a weight $w_e\in[0,1]$, such that node $B$ recieves the following combination of the original value $v_A$ and the ablated value $\mu_A$ from node $A$:

\begin{equation}
    w_ev_A + (1-w_e)\mu_A
\end{equation}

After the optimization is finished, the edges whose weight does not surpase a specified threshold are ablated.

If $\mathcal{D}$ is set to represent a biased behavior, and TEA is capable of effectively removing it from the model, then that would comfirm that $M$ computes the specified bias. Moreover, if combined with a method to analize the representations, it could help to identify what is the bias association being computed by $M'$, by comparing the inner representations before and after the ablation.\cite{geometry-of-truth}

\section{Example Generation}
\label{sec:examples}

\subsection{Counterfactual Examples}
\label{sec:examples:counterfactual}

In machine learning, a counterfactual explanation is an example that illustrates how a different input would result in a different output~\cite{NLP-posthoc-interpret-survey}. In NLP task, such as text classification or next token prediction, for instance, if an input text $X$ gets an output $y$ by the model, a counterfactual example would be any text $X'$, similar to $X$, that yields a different output $y'$~\cite{counterfactual-explanations,NLP-counterfactual-survey}. Though this definition is clear and widely used, it may be too narrow, as it leaves out the sense probability in the predictions.

\subsubsection{POLYJUICE}

The POLYJUICE~\cite{polyjuice} method employs a fine-tuned GPT-2~\cite{GPT-2} model to generate counterfactual examples. The model recieves an input text $X$ and a perturbation instruction, such as negation, insertion or deletion, and returns a modified version of $X$, following the given instruction.

\subsubsection{MiCE}

The Minimal Contrastive Editing (MiCE)~\cite{MiCE} method generate counterfactual examples via a masked language model, called editor model, that is trained to fill the masked spaces in an intput text with tokens, such that the resulting text would obtain a given prediction by some predictive model $M$. The editor model recieves the masked text and the target label as inputs. During training, the top $n_1\%$ tokens with highest gradient attribution~\cite{gradient-attribution}, towards the target prediction, are masked. To generate the counterfactuals, the percentage of masked tokens is varied between $0\%$ and $55\%$, using binary search to find the optimal percentage and beam search to keep track of the edits. The generation process stops once an edit changes the prediction to the target.

\subsubsection{GYC}

The GYC~\cite{GYC} method generates $k$ counterfactual examples, for a given input text $X$ and a condition $C$, through controlled text generation, without requiring to train or fine-tune a model. The method consists in modelating the distribution $p(\tilde{y}|X, C)$, where the condition $C$ can be any restriction over the text, such as a class label.

Let $LM$ be a language model transformer; $LM$ generates a token $y_t$ conditioned on the past tokens $y_{<t}=\{y_i\}^{t-1}_{i=0}$ as follows:

\begin{equation}
    \begin{split}
        o_{t}, H_{t} &= \text{LM}(y_{t-1}, H_{t-1})\\
        y_t &\sim \text{Categorical}(o_t)
    \end{split}
\end{equation}

where $H_{t-1}$ is the history matrix that captures the dependency of $y_{t-1}$ on past tokens and $o_t$ are the logits to sample $y_t$ from a categorical distribution. To generate text conditioned on $C$, $H$ is perturbed two times, first to create $\tilde{H}_t$ which enforces the reconstruction of $X$, and next to create $\hat{H}_t$ which enforces the condition. To learn the perturbations a linear combination of three loss functions is employed, one for reconstruction and one enforce the condition, plus another one to ensure diversity. The reconstruction loss maximizes the log probability of the input text, the condition loss maximizes a score assosiated to the condition and the diversity loss maximizes the entropy of the generated logits.


\subsection{Activation Maximization}
\label{sec:examples:activation_max}



\subsubsection{Preferred Input Synthesis}

The Preferred Input Synthesis (PIS)~\cite{synthetisizing-Nguyen} method generates inputs that maximize the activation of a target neuron (including the output logits), via optimization at the level of the latent space of a generative model. Given a predictive model $M:X\to Y$ and a generative model $G:Z\to X$, denoting by $M_h$ the activation of the target neuron $h$, PIS generate samples by solving:

\begin{equation}
    \argmax_{z\in Z}M_h(G(z)) - \lambda\|z\|
\end{equation}

where $\lambda$ is a regularization term and the optimization is performed with gradient descend. PIS was originally formulated for image generation, but can easily be re-adapted for text.


\subsubsection{Momentum Evolutionary Input Optimization}

The Evolutionary Input Optimization (MEIO)~\cite{synthetisizing-Barbalau} method is a generic framework for Activation Maximization that propose a model-agnostic approch, with a zero-order optimization on the latent space of a generative model. Given a predictive model $M:X\to Y$, a generative model $G:Z\to X$, and a target prediction $y\in Y$, MEIO generate samples by solving:

\begin{equation}
    \min_{z\in Z}\mathcal{L}(M(G(z)), y)
\end{equation}

where $\mathcal{L}$ is a loss function. The optimization is performed via an evolutionary strategy with momentum updates, that involves iteratively updating a set of candidate solutions, by adding to them an idenpdently sampled gaussian noise, which is combined with noised added in the previous itariton (the momentum).

